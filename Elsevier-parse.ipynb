{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is for parsing Elsvier papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "import re\n",
    "    \n",
    "## Load configuration\n",
    "con_file = open(\"config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "\n",
    "## Initialize client\n",
    "client = ElsClient(config['apikey'])\n",
    "# client.inst_token = config['insttoken']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pii_doc.title:  An AI-enhanced pattern recognition approach to temporal and spatial analysis of children's embodied interactions\n"
     ]
    }
   ],
   "source": [
    "pii_doc = FullDoc(sd_pii = 'S2666920X23000255')\n",
    "if pii_doc.read(client):\n",
    "    print (\"pii_doc.title: \", pii_doc.title)\n",
    "    pii_doc.write()   \n",
    "else:\n",
    "    print (\"Read document failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"section\": \"1 Introduction\", \"content\": \" Artificial Intelligence (AI) has tremendous potential to support educational research. That potential stems from the way AI can support and extend a researcher's ability to detect important features of learning when analyzing video data (Cukurova, Luckin, & Clark-Wilson, 2019). A number of researchers have successfully reported using AI to identify behavioral and physiological patterns in video and audio data that manifest student learning (e.g., Andrade et al., 2016; Di Mitri et al., 2017; Luckin & Cukurova, 2019). Others have similarly utilized AI-enhanced video analysis techniques to explore the relationship between embodied human behaviors and the underlying learning processes (Abrahamson et al., 2021; Cukurova et al., 2018; Lee-Cultura et al., 2022; Worsley et al., 2021). The general idea is that AI can help researchers identify patterns of behavior that reveal how human cognition and learning take place. One of the challenges with AI-enhanced video analysis is that the analysis of video data is, at its core, a human endeavor; it requires a human to interpret the meaning behind various modes of interaction (e.g., verbal and non-verbal) and capture both the temporal and spatial qualities of that meaning-making process (Bezemer, 2014; Kendon, 1982). A current and pressing question among qualitative researchers, then, is whether and how AI can play a productive role in supporting the human analysis of qualitative data (Feuston & Brubaker, 2021; Marathe & Toyama, 2018; Sharma et al., 2019). The overall goal is to find a role for AI that does not replace humans in the process but instead serves as a tool for augmenting the analysis and interpretation of video data (Jiang et al., 2021). Research on AI-enhanced techniques for analyzing young children's learning is becoming increasingly popular because video is minimally invasive and provides a low-cost alternative to collecting high-frequency data (Crescenzi-Lanna, 2020; Reilly et al., 2018). Additionally, combining AI with video analysis allows researchers to study diverse aspects of children's learning in greater detail than human analysis alone while easing the challenges of analyzing complex classroom interactions that evolve over time (Crescenzi-Lanna, 2020). Overall, these perspectives support the idea that AI has the potential to improve the analysis of video data. At the same time, a growing number of scholars have suggested that additional studies are needed to determine the role that AI can play in analyzing video data (Andrade et al., 2016; Ramakrishnan et al., 2019). Thus, there is a current and pressing need for studies that detail the ways AI can support human analysis when analyzing the meaning behind human behavior in captured through video data (Abrahamson et al., 2021; Feuston & Brubaker, 2021; Marathe & Toyama, 2018; Sullivan & Keith, 2019; Worsley & Blikstein, 2018). In this study, we explored how a supervised approach to training a deep neural network could augment the human analysis of multimodal video data. The study builds on our previous and ongoing work with AI-enhanced multimodal analysis (Ocak et al., 2021) to demonstrate how a machine-learning algorithm could ease the burdens of human analysis by automating the process of identifying different types of interactions within a set of video data. In our approach, we trained an AI to identify five different modes of interaction using a set of images from our validation set; those modes were established using prior research. We then applied the AI to a 5-min segment of video data to validate the AI and analyze how multiple modes of interaction occurred simultaneously (e.g., talking while gesturing; moving one's body while using objects in the environment). The following research question guided our study: 1. How can we develop a supervised machine learning (ML) algorithm that can support the human analysis of pre-identified modes of interaction evidenced in video data? 2. How can the AI output support humans in transcribing multimodal video data? Our process for exploring the guiding questions is detailed below. We first review literature from the fields of multimodal video analysis and AI. We then describe our process of extracting images and training the AI to engage in video analysis. In our results, we present the results of our confusion matrices that evaluated the performance of the AI within each of the modes of interaction. Finally, we discuss the results and explore how a researcher might use AI to support multimodal analysis in the future. 2 Related work\"}, {\"section\": \"2 Related work\", \"content\": \" Analyzing video data in educational research is both time-consuming and burdensome. It requires a researcher to view video data multiple times; with each viewing, the researcher is working to identify key features that reveal moments or episodes in which the participant is engaging in learning or cognition (Bezemer & Jewitt, 2010). Identifying key features and specific moments of activity is an inductive process in which the researcher first labels key behaviors or interactions, iteratively develops categories or themes from those labels, and then selects key moments from the video that can provide deeper insight into the phenomenon of interest (Bezemer, 2014). This process can take weeks or even months, depending on the size of the research team and the rate at which the team is able to label the video data and identify key moments within that data. AI has the potential to ease the burdens associated with the human analysis of multimodal video data. As noted by Marathe and Toyama (2018), a researcher can generate labels that have been established on a subset of data and use them to classify key behaviors within a larger, unseen data set. This can save a researcher significant amounts of time and resources when analyzing multimodal interactions through video data. One way educational scholars have used AI to detect patterns of human interaction in video data is with unsupervised methods. Unsupervised AI models are those in which the AI produces an analysis, but the researcher does not know or have input into how the analysis took place (Star\\u010di\\u010d, 2019). For example, Andrade et al. (2016) used an unsupervised approach to analyze the behavioral frames of video-recorded interviews and identify clusters of learning behavior that were socially meaningful. Detecting the micro-shifts in learners\\u2019 behavioral frames, the authors were able to provide one approach in which AI supported the analysis of qualitative data that is large, nuanced, and complex. Reilly et al. (2018) similarly used clustering algorithms to analyze learners' movements, facial expressions, and speech through KinectTM that emerged during collaborative group interactions. The authors found that specific movements and gesture patterns positively correlated with the quality of collaboration among small learning groups. One of the biggest concerns over unsupervised methods, however, is that unsupervised methods typically do not account for the way that theory and a priori classification can be used to drive the AI model (Andrade et al., 2016; Star\\u010di\\u010d, 2019). In the context of qualitative multimodal analysis, this lack of transparency is problematic. Multimodal analysis has been and will always be an interpretive act on the part of the researcher; it requires decisions about what pieces of data a researcher should focus on to reveal the meaning behind the actions and words captured in the data (Bezemer, 2014). This is because social interactions are highly contextual; to make sense of the meaning of social interactions, one must determine how various modes are coming together in a given moment to reflect human reasoning. This requires attention to both the spatial and temporal aspects of the data\\u2013how an interaction emerges, changes, and develops over time to support meaning-making in a given context (Qin et al., 2015). The validity of any multimodal analysis, then, is dependent on high levels of transparency throughout the process; a researcher must clearly articulate how decisions are made about the modes that are being analyzed and the way that those modes reveal the construction of meaning (Bezemer, 2014; Mondada, 2018). Overall, these perspectives suggest that AI-enhanced multimodal analysis of video data must go beyond identifying patterns of behavior and include techniques that help researchers understand the meaning behind those patterns (Cukurova, Kent, & Luckin, 2019). Another way to approach AI-enhanced analysis of video data is through supervised methods. Supervised approaches are those in which a researcher uses a priori theory and human annotation of data to train the AI before using the AI to support pattern detection and prediction (Andrade et al., 2016; Star\\u010di\\u010d, 2019). Both Luckin and Cukurova's (2019) theory-driven AI design and Sharma et al.\\u2019s (2019) hypothesis-driven approach for feature extraction suggest how supervised approaches might work. A researcher first identifies and translates the basic analytic tasks into a form that can be delegated to the AI. This is typically accomplished by having researchers train the AI using data that contains the features that portray certain aspects of learning behavior (Martin & Sherin, 2013). The general idea is that the researcher is the one who first decides what does and does not constitute a mode or feature of interest, increasing the likelihood that the AI is detecting modes and features in a way that is meaningful to the researcher and relevant to the research questions (Luckin & Cukurova, 2019). The researcher then supervises the output of the AI analysis to ensure that the AI is labeling visual data in a way that is consistent with the underlying theory (Spikol et al., 2017). The process often entails a \\u2018back-and-forth\\u2019 dynamic in which the researcher trains the AI, evaluates the output, and adjusts the training data set until achieving an acceptable level of accuracy from the AI (Sharma et al., 2019). Thus, a supervised approach not only improves the performance of the AI model but also helps establish that the AI is accurately detecting the various modes of interaction that are present in the data. For example, Spikol et al. (2017) collected high-frequency data from multiple modalities, such as face tracking, hand tracking, and interactions with the visual programming interface, Arduino, to study learners' group interactions. By establishing the modes of interest prior to training the AI, the authors were able to focus on analyzing a targeted set of behaviors (e.g., the distance between hands and face) that were likely to be indicators of success when children were working on collaborative projects. 2.1 Research gap\"}, {\"section\": \"2.1 Research gap\", \"content\": \" Research that leverages modern deep perceptual architectures is understudied and needed both in the field of ML and education research (Ramakrishnan et al., 2019). For example, a recent review of AI literature by Chen et al. (2020) noted \\u201ca lack of studies that both employ AI technologies and engage deeply with educational theories.\\u201d Crescenzi-Lanna (2020) similarly noted how additional studies are needed to establish the potential for AI to complement the human analysis of children's learning captured in multimodal video data. Other scholars have offered specific areas of AI research that need attention. Ramakrishnan et al. (2019) offered a multi-modal machine-based learning system to determine whether a classroom climate was positive or negative through participants' facial expressions. The major challenge the authors faced was that it was difficult for the AI to label their data, given the complexity of the classroom interactions. Based on their challenges, the authors suggested that future research explore how a combination of techniques like multi-task learning and supervised pre-training might improve the way AI detects interactions of interest from video data. Sharma et al. (2019) used a pre-trained deep neural network to detect facial expressions in order to investigate the impact of emotions on group collaborations. Despite reporting success, the authors recommended future research focus on using AI to detect evidence of learning through additional modalities (e.g., dialogue, gesture) to improve the accuracy of the model. The authors further recommended comparing the results with manually compared data. These studies and others like them (e.g., Crescenzi-Lanna, 2020; Feuston & Brubaker, 2021; Spikol et al., 2017) support the need for research that explores how humans and machines can complement one another in the collection and analysis of multimodal video data. 3 Method\"}, {\"section\": \"3 Method\", \"content\": \" This study employed a human-in-the-loop approach to machine learning. As described by Monarch (2021), a human-in-the-loop approach is a supervised approach in which a human first labels a set of data that is used to train the AI. Once the AI has been trained, the model can be tested on a larger, related set of data, and the results can be validated by a team of humans. Monarch explained how this approach is well suited for generating machine learning algorithms that serve a specific purpose for a specific task and data set. The level of human involvement results in higher levels of accuracy in less time while improving confidence in the validity of the AI output (Monarch, 2021; Mosqueira-Rey et al., 2023). With this in mind, the algorithm developed in this study was trained using a pre-labeled training set and then tested on a validation set of data. Because this approach to developing an AI carries a risk of overfitting (Raschka, 2018), the performance of the algorithm was calculated using a confusion matrix that compared AI and human analysis on the validation set of data. Confirming the AI performance through human intervention added a measure of confidence to the accuracy and precision of the model associated with each mode of interaction (Monarch, 2021). Our process of developing and testing the AI is described fully below. 3.1 A priori theory\"}, {\"section\": \"3.1 A priori theory\", \"content\": \" Previous research by the authors (Kopcha et al., 2021; Kopcha & Ocak, 2019) identified the modes by which computational thinking (CT) was embodied by two 5th-grade students during an educational robotics activity. In those studies, multimodal analysis was used to analyze a 60-min segment of video that was collected from a rural school in the Southeastern US. In the video, two 5th-grade participants engaged in Danger Zone (Kopcha et al., 2017), which is a multi-day unit in which 5th-grade learners program and navigate a robot across a 3'x3\\u2032 grid of obstacles. The 60-min video contained footage of the two 5th-grade learners using both verbal and non-verbal (i.e., gestures) to engage in the robotics challenge. One of the primary outcomes of the prior multimodal analysis was that we identified five modes of interaction through which our participant's learning was evidenced: numerical representations that represented participants' use of their hands to indicate a numerical value, imitating the robot that represented participants' use of their own bodies to simulate the robot's movement, use of the workbook that represented participants' use of the workbook to program the robot, use of the computer that represented participants' activity at the computer, and the use of the robot itself that represented participants' physical interaction with the robot. Those modalities were identified as the result of analyzing different moments of interaction captured throughout the 60-min video to understand the phenomenon of interest. Table 1 in section 3.2. presents a summary of the five modes and their descriptions. The focus on these modes of interaction was purposeful and grounded in well-established theories of learning. Multimodal perspectives suggest that meaning is constructed through human interaction, including interaction with other people as well as the structures in the environment (Bezemer & Jewitt, 2010). In the context of educational robotics, multimodality offers a new way of analyzing and assessing the development of children's thinking while engaging in computer programming (Kopcha et al., 2021; Zhang & Nouri, 2019). While much is known about specific CT skills, far less is known about how those skills develop within specific instructional activities over time (Tang et al., 2020). For example, gestures are one prominent mode of interaction that they can reveal how children use their bodies to visualize complex mathematical ideas when engaging in problem-solving activity (Kopcha et al., 2021; Bakala et al., 2021; Su & Yang, 2023). The modes of interaction used in this study\\u2013numerical representations, imitating the robot, and tool use (e.g., computer, robot, workbook)\\u2013represent key modalities through which children's CT can manifest. Studying children's CT through their use of modes addresses a noted gap in the literature that examines how children use their bodies and various tools in the environment to develop CT at a young age (Chen et al., 2017; Su & Yang, 2023). 3.2 Data set\"}, {\"section\": \"3.2 Data set\", \"content\": \" For the current study, we returned to the same video data used in our previous studies and focused on a 5-min segment; all research activity was reviewed and approved by our University's Internal Review Board prior to conducting the research. The 5-min segment of video data was used to generate both a training and validation set of data for the AI. This 5-min segment was chosen because of the way it included visible moments in which the participants interacted with each other through the five modes we had previously identified. Our goal was to train the AI to engage in feature extraction so it could identify those previously established modes summarized in Table 1. To develop the data set, we used an image extraction program to extract 30 frames per second from the 5-min video. This resulted in 9000 images in the data set. To ensure quality and consistency among the images, we resized the extracted images to 128 \\u00d7 128 resolution and normalized the intensity in all the red, blue, and green channels between 0 and 1. 3.3 Network design\"}, {\"section\": \"3.3 Network design\", \"content\": \" We designed a minimalist Convolution Neural Network (LeCun & Bengio, 1995 ), shown in Fig. 1 , consisting of one block with two convolutional layers with feature maps of size 32. The filter size used was 3 \\u00d7 3. Each convolution layer used the ReLu activation function and was followed by batch normalization. Next, we flattened the output of the last convolution layer and connected it to the dense block of size five, representing five possible classes. The dense layer used the sigmoid activation function to bring a multi-level approach to the underlying model and develop robust decision-making boundaries (Kang, 2017). Finally, we used the Adam Optimizer with a learning rate of 0.00005; this approach is a common method for optimizing the learning rate of an AI (Kingma & Ba, 2014). The input to the network 2 2 The network used in this study is available on GitHub: https://gitfront.io/r/user-8254535/RsU1BQqLSnwj/Paper-gitfront/. was a single image, and the output of the network was the probability of the different classes. 3.4 Training\"}, {\"section\": \"3.4 Training\", \"content\": \" To train the AI, we reviewed the 9000 images in the entire data set and selected 75 of those images; those images represented the behavior of interest in each of the five modalities (15 images per mode). The images were selected such that they showed each mode at various points across the 5-min video segment. Fig. 2 contains a sample image of a representative behavior for each of the five modalities, including looking off-camera at the computer (Fig. 2a), gesturing a numerical representation (Fig. 2b), imitating the robot's movement (Fig. 2c), looking at workbook (Fig. 2d) and interacting with the robot (Fig. 2e). These images were selected because they fit the description of each mode identified through a priori theory (see Table 1). In order to reduce the complexity of labeling all possible labels for an image of the video frame, the training set was simplified to keep only the most apparent label attributed to it. Specifically, we used a single image per class for validation during training. Using a single image per class reduced the complexities of labeling, which helped prevent the network from overfitting due to the small size of the dataset (Brownlee, 2019). Our training took several trials. In the first trial, the AI did not label all the interactions as expected. To improve the performance of our AI, we increased the number of images in each category to 25 (125 images total) while also refining our image selection so that our images clearly displayed the behavior of interest. While it is generally recommended that 100 or more images are needed to train an AI, researchers have reported success with as few as 25 images when those images are drawn from a larger related data set (see Schouten, Matek, Jacobs, & et al., 2021). In our second trial, the AI yielded 80% accuracy of a match with our manual classification of the training data set. This was achieved by allowing the AI to run the training data for multiple cycles and identifying where the performance dropped off. In our case, it took eight cycles/epochs to achieve 80% accuracy. Across ten different runs, the network was able to achieve 80% accuracy on 7 out of 10 runs. In the remaining three runs, the network obtained a performance of 70% accuracy. We selected the last run in which we obtained 80% accuracy as the model for further tests. The 20% inaccuracy resulted from some images lacking distinct, unique features altogether or a single image resulting in multiple possible categorization outcomes (e.g., one student looks at the computer while the other imitates the robot). 3.5 Testing and validation\"}, {\"section\": \"3.5 Testing and validation\", \"content\": \" After training, we tested the AI using the entire set of 9000 images extracted from the 5-min segment of the video. During testing, as with all classification pipelines (Sharma et al., 2018), the network outputs the probability of every possible label. A threshold of 0.5 was chosen to represent a true value for the particular label given an input image; this threshold is recommended as the starting point for model validation when using a training set (Raschka, 2018). This would essentially accept the labels for which the network had at least 50% confidence, which is an example of the default cut-off value of a classifier (Harrison, 2021). While some approaches to validating machine learning recommend iteratively improving the threshold using techniques such as a precision-recall graph, we kept the threshold setting to the default value of 0.5. Our decision was based on the fact that the performance at this threshold was acceptable for the purpose of using the AI to classify images that were later validated through human analysis. To validate the AI results, the AI was configured to automatically annotate the images so that one form of output was a movie consisting of each analyzed image with a display of the interaction type detected in each image. We selected 300 of the 9000 annotated frames, meaning that we selected every 30th frame (i.e., \\u223c1 image per second) so that two of the researchers could determine whether the AI had labeled the images correctly. We chose this rate because humans often view video data on a second-by-second basis when analyzing it for multimodal interactions (Mavers, 2012). This helped establish our process as a human-in-the-loop approach to machine learning in that the contributions of the AI were strongly aligned with the needs and limitations of the human aspects of multimodal analysis (Monarch, 2021). Fig. 3 contains two of the annotated frames produced by the AI. The left image displays an example of an accurate classification by the AI; the girl is looking off-screen to the left at the computer (Com), and the boy is imitating the robot (IR). The right image displays a misclassification in which AI detected an interaction with the computer (Com) that did not occur. The misclassification was largely due to ambiguity regarding whether the boy was looking downward at the workbook (WB) or if he was looking off-screen to the left at the computer (Com). To answer our first question (i.e., How can we develop an AI), we calculated a confusion matrix for each of the five interactions that the AI was trained to identify. A confusion matrix is a tool for understanding the performance of an AI that provides researchers with information about the AI's ability to distinguish one classification from the other classifications (Tiwari, 2022). It is a 2 \\u00d7 2 matrix where each cell contains a value indicating whether the AI-predicted values reflect the presence (i.e., true positive or TP) or absence (i.e., true negative or TN) of a feature in the actual data set. The matrix also notes when an AI detects a feature that is not actually present in the data (i.e., false positive or FP) as well as instances when the AI does not detect a feature that is present (i.e., false negative or FN). The values contained in the cells of the confusion matrix are then used to calculate indicators of performance associated with the AI. As explained by Tiwari (2022), Accuracy reflects the overall amount of correct predictions; it is calculated as the total number of AI/human agreements divided by the total number of evaluations: A c c u r a c y = T P + T N T P + T N + F P + F N Precision reflects how well the model did in making positive predictions compared to all possible positives while Recall reflects how often those positives were accurately predicted: P r e c i s i o n = T P T P + F P R e c a l l = T P T P + F N An F1 value accounts for whether similar levels of precision and recall were achieved: F 1 = 2 \\u00d7 P r e c i s i o n \\u00d7 R e c a l l P r e c i s i o n + R e c a l l Overall, the calculations generated from each confusion matrix provide a robust indication of the performance of the AI because they account for both correct and incorrect predictions made by the AI (Tiwari, 2022). To generate values for our confusion matrices, two of the researchers independently reviewed each of the 300 annotated frames. The goal was to confirm whether the AI had correctly (i.e., True Positive and True Negative) or incorrectly (i.e., False Positive) identified the interaction displayed in the image or whether additional interactions were missing (i.e., False Negative). To ensure the validity of the human analysis, a percent agreement was calculated to establish whether the researchers agreed on the AI's labeling of images. The agreement between reviewers was 95%, suggesting that the two researchers had a very high level of agreement about the AI's classification of images. To answer our second research question (i.e., How can the output support human analysis), we used the true positive rate to establish the frequency with which the participants engaged in each mode of interaction. Because each image reflected 1 s of activity in the video, this frequency analysis provided insight into the modes of interaction that were most dominant. In multimodal analysis, however, the frequency with which a mode occurs may not reflect the importance or meaning of that mode in human reasoning (Bezemer & Jewitt, 2010). To better understand the meaning behind the modes of interaction, we used the AI output to determine where distinct shifts in modes took place (see Fig. 4 a in Section 4). Specifically, we exported the AI classification for each annotated frame into a spreadsheet in which the timing of the image in the sequence (e.g., 0001, 0002) could be paired with the presence of different modes of interaction. This display allowed us to visually inspect the participants' use of modes over time and identify any moments in which those modes shifted; this is important because a shift in modes can reflect a change or development in their thinking and learning (Goffman, 1974). We then used Bezemer's (2014) recommendations for multimodal analysis to analyze the meaning that was reflected through our participants' modes of interaction. Specifically, we generated a multimodal transcript of our participants' interactions at a key moment in which they began using numerical representations as part of their CT. We first paired participant discourse with images of their interactions, including the labels generated by the AI. We then developed larger themes and categories that reflected the meaning behind those labels. This added to the researcher's interpretation of the meaning behind those behaviors in the context of CT. 4 Results\"}, {\"section\": \"4 Results\", \"content\": \" To answer our first research question (i.e., How can we develop a supervised machine learning (ML) algorithm), we generated a confusion matrix to evaluate the performance of the AI on each of the five modes of interaction. Table 2 presents the results from the metrics calculated on each confusion matrix. The table displays the instances in which the AI prediction was consistent with human analysis (i.e., True Positive or TP, True Negative or TN), instances when the AI predicted a result that was not present (i.e., False Positive or FP), and instances when the AI did not detect an interaction that was present (i.e., False Negative or FN). Overall, the model showed levels of accuracy greater than 80% for four of the five modes of interaction: Computer, Imitating the Robot, Numerical Representations, and Interacting with the Robot. Accuracy for the Workbook mode was 74%, which was the lowest accuracy across the modes. Precision and Recall were highest for Computer (83% and 93%, respectively) and Numerical Representations (77% and 88%, respectively); this was reflected in the F1 values for Computer and Numerical Representation, which were also the highest at 88% and 82% respectively. The lowest levels of Precision and Recall were found for Imitating the Robot (64% and 47%) and for Interacting with the Robot (100% and 20%); likewise, the F1 values for Imitating the Robot and Interacting with the Robot were lowest at 54% and 33% respectively. The second research question explored how the AI output could be used to support humans in analyzing multimodal video data. To answer this question, we first displayed the modes of interaction identified by the AI in an image-by-image fashion so that shifts in modes could be more easily identified through visual inspection of the AI output. Fig. 4a displays a segment of our spreadsheet in which the image number is noted in the first column, and the corresponding presence of an interaction is noted in the second (e.g., Com = use of computer; Num = Numerical Representation). The True Positive (TP) rate displayed in Table 2 provides a frequency count of the number of times our participants spent engaged in each mode of interaction. Because each image reflects 1 s of our video, the frequencies were converted to time in order to reflect the amount of time spent engaging in each mode. Participants spend the most time interacting with the Computer (3.68 s) and Workbook (1.71 s) and using their hands to make numerical representations (0.83 s). While frequencies are important, multimodal researchers are often more interested in understanding how modes shift over time and the human reasoning behind those shifts (Bezemer, 2014; Mondada, 2018). To identify such shifts, we performed a visual inspection of the modes of interaction displayed in our spreadsheet. Because interactions with the computer (COM) and workbook (WB) were among the most frequent, the visual inspection focused on instances in which other modes of interaction appeared. One evident pattern was that numerical representations would occur in groups - the participants would use them for a period of time, then stop using them for a period of time. To better understand this pattern, we isolated one specific moment in which numerical representations played a key role in our participants' CT. The boy is using four fingers to represent the time setting. At the same time, the girl is explaining the role that the time setting plays in programming the robot: \\u201cEverything is the same besides the time!\\u201d This use of numerical representation suggests that the participants are applying the relationship between distance, speed, and time to their thinking as they program the robot. Because the participants had no prior understanding of this relationship in the context of computer programming, the boy's use of four fingers reveals a critical aspect of the way their CT was associated with their bodily movement. 5 Discussion\"}, {\"section\": \"5 Discussion\", \"content\": \" The primary goal of this study was to contribute to a developing body of literature in which there is a noted need for studies that use a priori theory to guide the development of an AI model that can aid human analysis of learning (see Luckin et al., 2016; Star\\u010di\\u010d, 2019). In terms of our first research question, we designed and developed a theory-driven framework for training the AI to engage in feature detection. We paired a holdout method of validation with human validation of AI output to increase confidence in the validity of the model and the output (Keevers, 2019). We then established the performance of the AI (i.e., our second research question). The results suggest we were successful in many ways. Our validation of the AI output suggests that the AI was able to identify with high levels of precision and recall our participant's interactions with the computer, workbook, and instances in which they used their hands and bodies to represent numbers. The F1 values for the computer and numerical representations, specifically, suggest that the AI was not only able to detect instances of each from non-instances but did so with high levels of precision. These findings support scholars who have suggested that an AI can be trained to engage in feature detection with high levels of accuracy when the approach is grounded in an existing theory and supervised by humans (Luckin & Cukurova, 2019; Sullivan & Keith, 2019). In terms of research on the use of AI in educational research, this type of study is important because it adds to a small but growing interest in using human intervention to train and validate AI to enhance feature detection in small, well-defined data sets (e.g., Nash, Zheng, & Birbilis, 2022; Schouten, Matek, Jacobs, & et al., 2021). The results of this study suggest that a human-in-the-loop approach can be one way to accomplish that goal. The F1 values also indicated that the AI struggled to detect when our participants were imitating the robot and when they were interacting with the robot. In both cases, the recall was low, which suggests that the AI was unable to consistently detect positive instances of each mode. This is likely due to the fact that the AI was trained with 25 images. Similar to our results, Schouten, Matek, Jacobs, & et al., 2021 found 25 images to be enough to train their AI to detect differences among two types of cells. However, the authors also noted how the AI was better able to detect specific regions of pixels when more images were used to train their model. It is likely, then, that the AI in our study had not yet learned to identify the specific regions within our images that indicated when our participants were imitating or interacting with the robot. Imitating the robot can take many forms, from holding one's hands out to turning, twisting, and even moving like the robot. Likewise, the robot often appeared as a black square in the images that lacked any distinct color or boundaries to help the AI distinguish it from other features within the image. This may have made it more difficult for the AI to make a positive prediction about those modes, given the manner in which the AI was trained. Despite the fact that the AI struggled to detect instances of imitating and interacting with the robot, the results are promising overall and provide insight into the ways that AI can be used to address ongoing challenges in multimodal research. One longstanding issue in multimodal analysis of video data deals with establishing validity and inter-rater reliability in one's research. As noted by Bezemer (2014) and others (Mondada, 2018), the strength of any multimodal analysis stems from using techniques that establish consistency among raters and an accurate depiction of the interactions that occurred. Those techniques are often overlooked by researchers because the human analysis of video data is both tedious and time-consuming, which diminishes the potential for multimodal analysis to play a role in better understanding human learning is diminished (Sullivan & Keith, 2019; Worsley, 2018; Worsley et al., 2021). The current study suggests that AI can help address this issue in multimodal research and play an important role in the analysis of multimodal data. In terms of our second research question, the AI served as an additional research team member whose role was to code the video data using five pre-determined codes. The human members of the team ensured agreement across ratings, cross-checking the AI output to ensure accuracy, precision, and validity. Our team then used those ratings to calculate statistics on the performance of our model. Once the performance of the AI was established, we used the output to engage in a detailed analysis of the multimodal video data in ways that went beyond human analysis alone. In our case, we were able to construct an overall idea of the amount of time the participants spent in each mode of interaction. We were also able to identify a key moment in which the AI detected a shift in our participants' use of numerical representations. Transcribing that moment revealed how a numerical gesture reflected a far deeper bodily understanding of the relationship between distance, rate, and time. In the context of educational research, this result is important because it demonstrates how children embody mathematics as part of their CT during educational robotics (see Kopcha et al., 2021). It also adds to a growing body of literature that suggests how children's gestures have been shown to play an important role in the learning of mathematics. Children's gestures can reveal how they are making sense of abstract mathematical concepts and support them in moving to more formal mathematical thinking in the future (Abrahamson & Trninic, 2015; Lakoff & N\\u00fa\\u00f1ez, 2000; Nathan, 2021). Our findings provide needed insight into the ways that AI can reduce the labor-intensive aspects of analyzing children's gestures in video data and support scholars in understanding how mathematics is grounded in one's body (Abrahamson et al., 2021). 5.1 Implications\"}, {\"section\": \"5.1 Implications\", \"content\": \" One implication is that AI can be developed through a human-in-the-loop approach to help assuage some of the time-consuming and labor-intensive aspects of multimodal video analysis. The AI produced frames that were annotated with labels indicating which modes of interaction were present. Additionally, those frames can be exported into a spreadsheet in which the timing of the image in the sequence (e.g., 0001, 0002) is paired with the presence of different modes of interaction. This has the potential to save a researcher time when analyzing video data. A researcher can easily use that output to identify key moments in the video data, return to the video, and transcribe both the verbal and non-verbal components of the moment to understand how human learning is taking place. This is important because identifying a shift in modes is an important step in multimodal analysis. A change in the frequency of a mode can indicate how learners are using the environment to support their thinking (see Bieda & Nathan, 2009; Vest et al., 2020). Our study suggests that AI-enhanced analytics can help human researchers better understand what the dominant mode of interaction might be, how it changes over time, and how the frequency of that mode compares to others (see also Andrade et el., 2017; Sullivan & Keith, 2019; Qin et al., 2015). 5.2 Limitations\"}, {\"section\": \"5.2 Limitations\", \"content\": \" A human-in-the-loop approach is meant to develop a machine-learning model with high levels of human monitoring and intervention to ensure that the model not only has strong performance but serves the needs of the humans who develop it. This typically requires the researcher to make decisions about the development of the model that balance the needs of human with the processes associated with machine learning (Monarch, 2021). For example, our method of validation was more aligned with holdout methods than a k-fold cross-validation technique. We also chose not to analyze the performance of the AI at different thresholds. These decisions may limit how the model can be applied more broadly because they may contribute to overfitting in our model (Raschka, 2018). It is important to note, however, that our goal was not to create an algorithm that could classify images universally. Our goal was to develop a machine learning model that could serve a specific purpose for a limited data set and support humans in multimodal analysis. Our process resulted in a machine learning model that served a variety of practical needs (i.e., lower in cost; greater human involvement) while achieving a level of performance that served the needs of the research team. 5.3 Future research\"}, {\"section\": \"5.3 Future research\", \"content\": \"\"}, {\"section\": \"6 Conclusion Funding Ethics statement Data availability statement\", \"content\": \"OTERO 2014 A LEARNINGBECOMINGINPRACTICEINTERNATIONALCONFERENCELEARNINGSCIENCESICLS2014 ADVANCINGEPISTEMOLOGICALFRAMEANALYSISREFINEUNDERSTANDINGINQUIRYFRAMESINEARLYELEMENTARYINTERVIEWS ANDRADE 2017 18 46 A ANDRADE 2016 282 306 A BACH 2015 1 30 S BAKALA 2021 1 20 E BEZEMER 2014 155 J BEZEMER 2010 180 197 J RESEARCHMETHODSINLINGUISTICS MULTIMODALANALYSISKEYISSUES BIEDA 2009 637 650 K BROWNLEE 2019 J HOWAVOIDOVERFITTINGINDEEPLEARNINGNEURALNETWORKSMACHINELEARNINGMASTERY CHEN 2017 162 175 G CHEN 2020 100002 X CRESCENZILANNA 2020 1485 1504 L CUKUROVA 2019 3032 3046 M CUKUROVA 2019 490 504 M CUKUROVA 2018 93 109 M DIMITRI 2017 188 197 D PROCEEDINGSSEVENTHINTERNATIONALLEARNINGANALYTICSKNOWLEDGECONFERENCE LEARNINGPULSEAMACHINELEARNINGAPPROACHFORPREDICTINGPERFORMANCEINSELFREGULATEDLEARNINGUSINGMULTIMODALDATA FEUSTON 2021 1 25 J PROCEEDINGSACMHUMANCOMPUTERINTERACTION5CSCW2 PUTTINGTOOLSINPLACEROLETIMEPERSPECTIVEINHUMANAICOLLABORATIONFORQUALITATIVEANALYSIS GOFFMAN 1974 E FRAMEANALYSISESSAYORGANIZATIONEXPERIENCE HARRISON 2021 G JIANG 2021 1 23 J KANG 2017 N MULTILAYERNEURALNETWORKSSIGMOIDFUNCTIONDEEPLEARNINGFORROOKIES2TOWARDSDATASCIENCE KEEVERS 2019 T JOINTOPERATIONSANALYSISDIVISION CROSSVALIDATIONINSUFFICIENTFORMODELVALIDATION KENDON 1982 A CONDUCTINGINTERACTIONPATTERNSBEHAVIORINFOCUSEDENCOUNTERS KINGMA 2014 D PROCEEDINGS3RDINTERNATIONALCONFERENCEFORLEARNINGREPRESENTATIONS ADAMAMETHODFORSTOCHASTICOPTIMIZATION KOPCHA 2017 31 44 T KOPCHA 2019 464 471 T AWIDELENSCOMBININGEMBODIEDENACTIVEEXTENDEDEMBEDDEDLEARNINGINCOLLABORATIVESETTINGS EMBODIMENTCOMPUTATIONALTHINKINGDURINGCOLLABORATIVEROBOTICSACTIVITY KOPCHA 2021 1987 2012 T LAKOFF 2000 G MATHEMATICSCOMESHOWEMBODIEDMINDBRINGSMATHEMATICSBEING LECUN 1995 1995 Y HANDBOOKBRAINTHEORYNEURALNETWORKS CONVOLUTIONALNETWORKSFORIMAGESSPEECHTIMESERIES LEECULTURA 2022 100355 S LUCKIN 2019 2824 2838 R LUCKIN 2016 R INTELLIGENCEUNLEASHED MARATHE 2018 M CHI18PROCEEDINGS2018CHICONFERENCEHUMANFACTORSINCOMPUTINGSYSTEMS SEMIAUTOMATEDCODINGFORQUALITATIVERESEARCHAUSERCENTEREDINQUIRYINITIALPROTOTYPES MARTIN 2013 511 520 T MONARCH 2021 R HUMANINTHELOOPMACHINELEARNINGACTIVELEARNINGANNOTATIONFORHUMANCENTEREDAI MONDADA 2018 85 106 L MOSQUEIRAREY 2023 3005 3054 E NASH 2022 26 W NATHAN 2021 M FOUNDATIONSEMBODIEDLEARNINGAPARADIGMFOREDUCATION OCAK 2021 273 278 C PROCEEDINGS29THINTERNATIONALCONFERENCECOMPUTERSINEDUCATIONASIAPACIFICSOCIETYFORCOMPUTERSINEDUCATION AIENHANCEDPATTERNRECOGNITIONAPPROACHANALYZECHILDRENSEMBODIEDINTERACTIONS QIN 2015 635 642 J 15PROCEEDINGS5THACMINTERNATIONALCONFERENCEMULTIMEDIARETRIEVALICMR TEACHINGVIDEOANALYTICSBASEDSTUDENTSPATIALTEMPORALBEHAVIORMINING RAMAKRISHNAN 2019 1 8 A 201914THIEEEINTERNATIONALCONFERENCEAUTOMATICFACEGESTURERECOGNITIONFG2019 TOWARDAUTOMATEDCLASSROOMOBSERVATIONPREDICTINGPOSITIVENEGATIVECLIMATE RASCHKA 2018 S MODELEVALUATIONMODELSELECTIONALGORITHMSELECTIONINMACHINELEARNING REILLY 2018 333 339 J INTERNATIONALEDUCATIONALDATAMININGSOCIETY11THINTERNATIONALCONFERENCEEDUCATIONALDATAMININGEDM2018 EXPLORINGCOLLABORATIONUSINGMOTIONSENSORSMULTIMODALLEARNINGANALYTICS SCHOUTEN 2021 7995 J SHARMA 2018 377 384 N SHARMA 2019 3004 3031 K SPIKOL 2017 D 2017MAKINGADIFFERENCEPRIORITIZINGEQUITYACCESSINCSCL12THINTERNATIONALCONFERENCECOMPUTERSUPPORTEDCOLLABORATIVELEARNINGCSCL2017 USINGMULTIMODALLEARNINGANALYTICSIDENTIFYASPECTSCOLLABORATIONINPROJECTBASEDLEARNING SPIKOL 2018 366 377 D STARCIC 2019 2974 2976 A SULLIVAN 2019 3047 3063 F SU 2023 100122 J TANG 2020 103798 X TIWARI 2022 A ARTIFICIALINTELLIGENCEMACHINELEARNINGFOREDGECOMPUTING SUPERVISEDLEARNINGTHEORYAPPLICATIONS VEST 2020 128 155 N WORSLEY 2018 M COMPANIONPROCEEDINGSEIGHTHINTERNATIONALCONFERENCELEARNINGANALYTICSKNOWLEDGELAK201859MARCH2018SYDNEYAUSTRALIA MULTIMODALLEARNINGANALYTICSPASTPRESENTPOTENTIALFUTURES WORSLEY 2018 385 419 M WORSLEY 2021 10 27 M ZHANG 2019 103607 L OCAKX2023X100146 OCAKX2023X100146XC Full 2023-06-04T06:30:22Z Author http://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY-NC-ND license. Published by Elsevier Ltd. item S2666-920X(23)00025-5 S2666920X23000255 1-s2.0-S2666920X23000255 10.1016/j.caeai.2023.100146 778123 2023-07-01T06:59:37.492535Z 2023-01-01 2023-12-31 UNLIMITED NONE 1-s2.0-S2666920X23000255-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/MAIN/application/pdf/b665ee7ec5f85c72810174faf7a624fd/main.pdf main.pdf pdf true 3591632 MAIN 10 1-s2.0-S2666920X23000255-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/PREVIEW/image/png/9f47be0f591d57c6dbf0f482725b1e4d/main_1.png main_1.png png 61345 849 656 IMAGE-WEB-PDF 1 1-s2.0-S2666920X23000255-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/gr1/DOWNSAMPLED/image/jpeg/7979a5df6669ca1fe34f2141c3aee227/gr1.jpg gr1 gr1.jpg jpg 100067 256 624 IMAGE-DOWNSAMPLED 1-s2.0-S2666920X23000255-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/gr4/DOWNSAMPLED/image/jpeg/fac4f21c7652a2508080b3b0c8e361d9/gr4.jpg gr4 gr4.jpg jpg 158755 463 713 IMAGE-DOWNSAMPLED 1-s2.0-S2666920X23000255-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/gr3/DOWNSAMPLED/image/jpeg/f77d5e745216a6dd37f34f6d5900397f/gr3.jpg gr3 gr3.jpg jpg 143770 296 713 IMAGE-DOWNSAMPLED 1-s2.0-S2666920X23000255-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/gr2/DOWNSAMPLED/image/jpeg/5f83ea14941d218a87a5f4bfe173981d/gr2.jpg gr2 gr2.jpg jpg 127430 200 713 IMAGE-DOWNSAMPLED 1-s2.0-S2666920X23000255-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/gr1/THUMBNAIL/image/gif/2eb8061083c894bec44e82c51916f2e2/gr1.sml gr1 gr1.sml sml 72877 90 219 IMAGE-THUMBNAIL 1-s2.0-S2666920X23000255-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/gr4/THUMBNAIL/image/gif/58b63347699c1876c7898d59af5fb8ce/gr4.sml gr4 gr4.sml sml 79539 142 219 IMAGE-THUMBNAIL 1-s2.0-S2666920X23000255-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/gr3/THUMBNAIL/image/gif/eb6a1909eb17200b840c3731cb5d92ee/gr3.sml gr3 gr3.sml sml 84381 91 219 IMAGE-THUMBNAIL 1-s2.0-S2666920X23000255-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/gr2/THUMBNAIL/image/gif/d3f5833c8383340fee5c7086e1826d6a/gr2.sml gr2 gr2.sml sml 76865 62 219 IMAGE-THUMBNAIL 1-s2.0-S2666920X23000255-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/HIGHRES/image/jpeg/26d80119c4a83e2a6b1d356c430de37f/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 287501 1133 2764 IMAGE-HIGH-RES 1-s2.0-S2666920X23000255-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/HIGHRES/image/jpeg/9748041d7ab50c78d22543c9699d6756/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 801611 2050 3158 IMAGE-HIGH-RES 1-s2.0-S2666920X23000255-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/HIGHRES/image/jpeg/0dc05d001f414d8b423a69e889ca1f25/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 721797 1310 3158 IMAGE-HIGH-RES 1-s2.0-S2666920X23000255-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/HIGHRES/image/jpeg/f819ba0972d8ef432c8a4a3207f0fd03/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 555122 888 3158 IMAGE-HIGH-RES 1-s2.0-S2666920X23000255-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/image/svg+xml/48c53b1ac7bfa7fd35b95ef11013a447/si1.svg si1 si1.svg svg 58224 ALTIMG 1-s2.0-S2666920X23000255-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/image/svg+xml/6532a4fc144d437f9fe128d237459e6c/si3.svg si3 si3.svg svg 52694 ALTIMG 1-s2.0-S2666920X23000255-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/image/svg+xml/a29315a6fd5631f56eb765f13750c348/si4.svg si4 si4.svg svg 72661 ALTIMG 1-s2.0-S2666920X23000255-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2666920X23000255/image/svg+xml/42c1e1c24042c978b5e5d121e6987d53/si2.svg si2 si2.svg svg 57168 ALTIMG 1-s2.0-S2666920X23000255-am.pdf am am.pdf pdf 633562 AAM-PDF https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/egi:10FSTW03LBT/MAIN/application/pdf/fef0d6c5863e76c97d4aefe87946fec3/am.pdf CAEAI 100146 100146 S2666-920X(23)00025-5 10.1016/j.caeai.2023.100146 Fig. 1 Network Diagram\\u2014The number of feature maps in each of the two convolution layers is 32, which is then flattened and followed by the 5 classification layers. Fig. 1 Fig. 2 A sample image for each mode of interaction. Fig. 2 Fig. 3 AI output (annotated images) displaying (a) accurate classification and (b) misclassification of interaction types. Fig. 3 Fig. 4 A Segment of Our Spreadsheet in Which The Timing of The Image in The Sequence Was Paired with The Presence of Interaction. Note: The spreadsheet displayed in Fig. 4a contains information for a subset of the 300 images that were analyzed by the AI and validated by the researchers. Fig. 4 Table 1 Five modes through which Children's CT occurs. Table 1 Pre-identified Modes Description Numerical representations Use of hands to indicate a numerical value (e.g., holding up four fingers for the number 4). Imitating the robot Use of body to simulate the robot moving forward (e.g., pulling hands apart), the robot's wheels when turning (e.g., rolling hands), and the robot pivoting when turning. Use of the workbook Use of a scaled-down map in a workbook to program the robot while away from the larger 3\\u2032 \\u00d7 3\\u2032 grid. Use of the computer Use of the computer to program the robot using block coding. Use of the robot itself Picking up or interacting with the robot. Note. The modes and descriptions were drawn from prior research by Kopcha & Ocak (2019) and Kopcha et al. (2021). Table 2 Summary of confusion matrix results for each of the five modes of interaction classified by the AI. Table 2 Modes TP FP FN TN Accuracy Precision Recall F1 Value Computer 221 44 17 18 80% 83% 93% 88% Numerical Represent. 50 15 7 228 93% 77% 88% 82% Workbook 103 55 23 119 74% 65% 82% 73% Imitating Robot 7 4 8 281 96% 64% 47% 54% Interact w/Robot 1 0 4 295 99% 100% 20% 33% An AI-enhanced pattern recognition approach to temporal and spatial analysis of children's embodied interactions Ceren Ocak a \\u2217 Theodore J. Kopcha b Raunak Dey c 1 a College of Education, Georgia Southern University, P.O. Box 7995, Statesboro, GA, 30460, USA College of Education Georgia Southern University P.O. Box 7995 Statesboro GA 30460 USA College of Education, Georgia Southern University, P.O. Box 7995, Statesboro, GA, 30460, USA b College of Education, University of Georgia, 850 College Station Rd., Athens, GA, 30605, USA College of Education, University of Georgia 850 College Station Rd. Athens GA 30605 USA College of Education, University of Georgia, 850 College Station Rd., Athens, GA, 30605, USA c University of Georgia, Burlingame, California, 94010, CA, USA University of Georgia Burlingame California CA 94010 USA Illumina, 200 Lincoln Centre Drive, Foster City, CA, 94404, USA \\u2217 Corresponding author. 1 Independent scholar Multimodal video analysis is a complex and time-consuming process for a researcher; it entails capturing, watching, and re-watching video data to identify which segments best inform or address the questions that drive the research. Modern AI applications can alleviate the challenges that arise during the fine-grained analysis of learners' multimodal interactions captured through video. In this study, we present a supervised approach to training a deep neural network to analyze children's computational thinking (CT) captured through multimodal video data. The approach first uses a set of images extracted from video data to train the AI to map them to labels generated using a priori theory. Confusion matrices were used to establish the performance of the AI by comparing AI predictions to human analysis on a validation set of data. The findings suggested that the AI classified several aspects of children's CT in a way that was highly consistent with human analysis, demonstrating how the AI could serve as an additional team member during multimodal analysis. Implications for using AI to ease the challenges of multimodal analysis of video data are discussed. Keywords Computational thinking (CT) Multimodality Embodied interactions Artificial Intelligence (AI) Machine learning 1 Introduction Artificial Intelligence (AI) has tremendous potential to support educational research. That potential stems from the way AI can support and extend a researcher's ability to detect important features of learning when analyzing video data (Cukurova, Luckin, & Clark-Wilson, 2019). A number of researchers have successfully reported using AI to identify behavioral and physiological patterns in video and audio data that manifest student learning (e.g., Andrade et al., 2016; Di Mitri et al., 2017; Luckin & Cukurova, 2019). Others have similarly utilized AI-enhanced video analysis techniques to explore the relationship between embodied human behaviors and the underlying learning processes (Abrahamson et al., 2021; Cukurova et al., 2018; Lee-Cultura et al., 2022; Worsley et al., 2021). The general idea is that AI can help researchers identify patterns of behavior that reveal how human cognition and learning take place. One of the challenges with AI-enhanced video analysis is that the analysis of video data is, at its core, a human endeavor; it requires a human to interpret the meaning behind various modes of interaction (e.g., verbal and non-verbal) and capture both the temporal and spatial qualities of that meaning-making process (Bezemer, 2014; Kendon, 1982). A current and pressing question among qualitative researchers, then, is whether and how AI can play a productive role in supporting the human analysis of qualitative data (Feuston & Brubaker, 2021; Marathe & Toyama, 2018; Sharma et al., 2019). The overall goal is to find a role for AI that does not replace humans in the process but instead serves as a tool for augmenting the analysis and interpretation of video data (Jiang et al., 2021). Research on AI-enhanced techniques for analyzing young children's learning is becoming increasingly popular because video is minimally invasive and provides a low-cost alternative to collecting high-frequency data (Crescenzi-Lanna, 2020; Reilly et al., 2018). Additionally, combining AI with video analysis allows researchers to study diverse aspects of children's learning in greater detail than human analysis alone while easing the challenges of analyzing complex classroom interactions that evolve over time (Crescenzi-Lanna, 2020). Overall, these perspectives support the idea that AI has the potential to improve the analysis of video data. At the same time, a growing number of scholars have suggested that additional studies are needed to determine the role that AI can play in analyzing video data (Andrade et al., 2016; Ramakrishnan et al., 2019). Thus, there is a current and pressing need for studies that detail the ways AI can support human analysis when analyzing the meaning behind human behavior in captured through video data (Abrahamson et al., 2021; Feuston & Brubaker, 2021; Marathe & Toyama, 2018; Sullivan & Keith, 2019; Worsley & Blikstein, 2018). In this study, we explored how a supervised approach to training a deep neural network could augment the human analysis of multimodal video data. The study builds on our previous and ongoing work with AI-enhanced multimodal analysis (Ocak et al., 2021) to demonstrate how a machine-learning algorithm could ease the burdens of human analysis by automating the process of identifying different types of interactions within a set of video data. In our approach, we trained an AI to identify five different modes of interaction using a set of images from our validation set; those modes were established using prior research. We then applied the AI to a 5-min segment of video data to validate the AI and analyze how multiple modes of interaction occurred simultaneously (e.g., talking while gesturing; moving one's body while using objects in the environment). The following research question guided our study: 1. How can we develop a supervised machine learning (ML) algorithm that can support the human analysis of pre-identified modes of interaction evidenced in video data? 2. How can the AI output support humans in transcribing multimodal video data? Our process for exploring the guiding questions is detailed below. We first review literature from the fields of multimodal video analysis and AI. We then describe our process of extracting images and training the AI to engage in video analysis. In our results, we present the results of our confusion matrices that evaluated the performance of the AI within each of the modes of interaction. Finally, we discuss the results and explore how a researcher might use AI to support multimodal analysis in the future. 2 Related work Analyzing video data in educational research is both time-consuming and burdensome. It requires a researcher to view video data multiple times; with each viewing, the researcher is working to identify key features that reveal moments or episodes in which the participant is engaging in learning or cognition (Bezemer & Jewitt, 2010). Identifying key features and specific moments of activity is an inductive process in which the researcher first labels key behaviors or interactions, iteratively develops categories or themes from those labels, and then selects key moments from the video that can provide deeper insight into the phenomenon of interest (Bezemer, 2014). This process can take weeks or even months, depending on the size of the research team and the rate at which the team is able to label the video data and identify key moments within that data. AI has the potential to ease the burdens associated with the human analysis of multimodal video data. As noted by Marathe and Toyama (2018), a researcher can generate labels that have been established on a subset of data and use them to classify key behaviors within a larger, unseen data set. This can save a researcher significant amounts of time and resources when analyzing multimodal interactions through video data. One way educational scholars have used AI to detect patterns of human interaction in video data is with unsupervised methods. Unsupervised AI models are those in which the AI produces an analysis, but the researcher does not know or have input into how the analysis took place (Star\\u010di\\u010d, 2019). For example, Andrade et al. (2016) used an unsupervised approach to analyze the behavioral frames of video-recorded interviews and identify clusters of learning behavior that were socially meaningful. Detecting the micro-shifts in learners\\u2019 behavioral frames, the authors were able to provide one approach in which AI supported the analysis of qualitative data that is large, nuanced, and complex. Reilly et al. (2018) similarly used clustering algorithms to analyze learners' movements, facial expressions, and speech through KinectTM that emerged during collaborative group interactions. The authors found that specific movements and gesture patterns positively correlated with the quality of collaboration among small learning groups. One of the biggest concerns over unsupervised methods, however, is that unsupervised methods typically do not account for the way that theory and a priori classification can be used to drive the AI model (Andrade et al., 2016; Star\\u010di\\u010d, 2019). In the context of qualitative multimodal analysis, this lack of transparency is problematic. Multimodal analysis has been and will always be an interpretive act on the part of the researcher; it requires decisions about what pieces of data a researcher should focus on to reveal the meaning behind the actions and words captured in the data (Bezemer, 2014). This is because social interactions are highly contextual; to make sense of the meaning of social interactions, one must determine how various modes are coming together in a given moment to reflect human reasoning. This requires attention to both the spatial and temporal aspects of the data\\u2013how an interaction emerges, changes, and develops over time to support meaning-making in a given context (Qin et al., 2015). The validity of any multimodal analysis, then, is dependent on high levels of transparency throughout the process; a researcher must clearly articulate how decisions are made about the modes that are being analyzed and the way that those modes reveal the construction of meaning (Bezemer, 2014; Mondada, 2018). Overall, these perspectives suggest that AI-enhanced multimodal analysis of video data must go beyond identifying patterns of behavior and include techniques that help researchers understand the meaning behind those patterns (Cukurova, Kent, & Luckin, 2019). Another way to approach AI-enhanced analysis of video data is through supervised methods. Supervised approaches are those in which a researcher uses a priori theory and human annotation of data to train the AI before using the AI to support pattern detection and prediction (Andrade et al., 2016; Star\\u010di\\u010d, 2019). Both Luckin and Cukurova's (2019) theory-driven AI design and Sharma et al.\\u2019s (2019) hypothesis-driven approach for feature extraction suggest how supervised approaches might work. A researcher first identifies and translates the basic analytic tasks into a form that can be delegated to the AI. This is typically accomplished by having researchers train the AI using data that contains the features that portray certain aspects of learning behavior (Martin & Sherin, 2013). The general idea is that the researcher is the one who first decides what does and does not constitute a mode or feature of interest, increasing the likelihood that the AI is detecting modes and features in a way that is meaningful to the researcher and relevant to the research questions (Luckin & Cukurova, 2019). The researcher then supervises the output of the AI analysis to ensure that the AI is labeling visual data in a way that is consistent with the underlying theory (Spikol et al., 2017). The process often entails a \\u2018back-and-forth\\u2019 dynamic in which the researcher trains the AI, evaluates the output, and adjusts the training data set until achieving an acceptable level of accuracy from the AI (Sharma et al., 2019). Thus, a supervised approach not only improves the performance of the AI model but also helps establish that the AI is accurately detecting the various modes of interaction that are present in the data. For example, Spikol et al. (2017) collected high-frequency data from multiple modalities, such as face tracking, hand tracking, and interactions with the visual programming interface, Arduino, to study learners' group interactions. By establishing the modes of interest prior to training the AI, the authors were able to focus on analyzing a targeted set of behaviors (e.g., the distance between hands and face) that were likely to be indicators of success when children were working on collaborative projects. 2.1 Research gap Research that leverages modern deep perceptual architectures is understudied and needed both in the field of ML and education research (Ramakrishnan et al., 2019). For example, a recent review of AI literature by Chen et al. (2020) noted \\u201ca lack of studies that both employ AI technologies and engage deeply with educational theories.\\u201d Crescenzi-Lanna (2020) similarly noted how additional studies are needed to establish the potential for AI to complement the human analysis of children's learning captured in multimodal video data. Other scholars have offered specific areas of AI research that need attention. Ramakrishnan et al. (2019) offered a multi-modal machine-based learning system to determine whether a classroom climate was positive or negative through participants' facial expressions. The major challenge the authors faced was that it was difficult for the AI to label their data, given the complexity of the classroom interactions. Based on their challenges, the authors suggested that future research explore how a combination of techniques like multi-task learning and supervised pre-training might improve the way AI detects interactions of interest from video data. Sharma et al. (2019) used a pre-trained deep neural network to detect facial expressions in order to investigate the impact of emotions on group collaborations. Despite reporting success, the authors recommended future research focus on using AI to detect evidence of learning through additional modalities (e.g., dialogue, gesture) to improve the accuracy of the model. The authors further recommended comparing the results with manually compared data. These studies and others like them (e.g., Crescenzi-Lanna, 2020; Feuston & Brubaker, 2021; Spikol et al., 2017) support the need for research that explores how humans and machines can complement one another in the collection and analysis of multimodal video data. 3 Method This study employed a human-in-the-loop approach to machine learning. As described by Monarch (2021), a human-in-the-loop approach is a supervised approach in which a human first labels a set of data that is used to train the AI. Once the AI has been trained, the model can be tested on a larger, related set of data, and the results can be validated by a team of humans. Monarch explained how this approach is well suited for generating machine learning algorithms that serve a specific purpose for a specific task and data set. The level of human involvement results in higher levels of accuracy in less time while improving confidence in the validity of the AI output (Monarch, 2021; Mosqueira-Rey et al., 2023). With this in mind, the algorithm developed in this study was trained using a pre-labeled training set and then tested on a validation set of data. Because this approach to developing an AI carries a risk of overfitting (Raschka, 2018), the performance of the algorithm was calculated using a confusion matrix that compared AI and human analysis on the validation set of data. Confirming the AI performance through human intervention added a measure of confidence to the accuracy and precision of the model associated with each mode of interaction (Monarch, 2021). Our process of developing and testing the AI is described fully below. 3.1 A priori theory Previous research by the authors (Kopcha et al., 2021; Kopcha & Ocak, 2019) identified the modes by which computational thinking (CT) was embodied by two 5th-grade students during an educational robotics activity. In those studies, multimodal analysis was used to analyze a 60-min segment of video that was collected from a rural school in the Southeastern US. In the video, two 5th-grade participants engaged in Danger Zone (Kopcha et al., 2017), which is a multi-day unit in which 5th-grade learners program and navigate a robot across a 3'x3\\u2032 grid of obstacles. The 60-min video contained footage of the two 5th-grade learners using both verbal and non-verbal (i.e., gestures) to engage in the robotics challenge. One of the primary outcomes of the prior multimodal analysis was that we identified five modes of interaction through which our participant's learning was evidenced: numerical representations that represented participants' use of their hands to indicate a numerical value, imitating the robot that represented participants' use of their own bodies to simulate the robot's movement, use of the workbook that represented participants' use of the workbook to program the robot, use of the computer that represented participants' activity at the computer, and the use of the robot itself that represented participants' physical interaction with the robot. Those modalities were identified as the result of analyzing different moments of interaction captured throughout the 60-min video to understand the phenomenon of interest. Table 1 in section 3.2. presents a summary of the five modes and their descriptions. The focus on these modes of interaction was purposeful and grounded in well-established theories of learning. Multimodal perspectives suggest that meaning is constructed through human interaction, including interaction with other people as well as the structures in the environment (Bezemer & Jewitt, 2010). In the context of educational robotics, multimodality offers a new way of analyzing and assessing the development of children's thinking while engaging in computer programming (Kopcha et al., 2021; Zhang & Nouri, 2019). While much is known about specific CT skills, far less is known about how those skills develop within specific instructional activities over time (Tang et al., 2020). For example, gestures are one prominent mode of interaction that they can reveal how children use their bodies to visualize complex mathematical ideas when engaging in problem-solving activity (Kopcha et al., 2021; Bakala et al., 2021; Su & Yang, 2023). The modes of interaction used in this study\\u2013numerical representations, imitating the robot, and tool use (e.g., computer, robot, workbook)\\u2013represent key modalities through which children's CT can manifest. Studying children's CT through their use of modes addresses a noted gap in the literature that examines how children use their bodies and various tools in the environment to develop CT at a young age (Chen et al., 2017; Su & Yang, 2023). 3.2 Data set For the current study, we returned to the same video data used in our previous studies and focused on a 5-min segment; all research activity was reviewed and approved by our University's Internal Review Board prior to conducting the research. The 5-min segment of video data was used to generate both a training and validation set of data for the AI. This 5-min segment was chosen because of the way it included visible moments in which the participants interacted with each other through the five modes we had previously identified. Our goal was to train the AI to engage in feature extraction so it could identify those previously established modes summarized in Table 1. To develop the data set, we used an image extraction program to extract 30 frames per second from the 5-min video. This resulted in 9000 images in the data set. To ensure quality and consistency among the images, we resized the extracted images to 128 \\u00d7 128 resolution and normalized the intensity in all the red, blue, and green channels between 0 and 1. 3.3 Network design We designed a minimalist Convolution Neural Network (LeCun & Bengio, 1995 ), shown in Fig. 1 , consisting of one block with two convolutional layers with feature maps of size 32. The filter size used was 3 \\u00d7 3. Each convolution layer used the ReLu activation function and was followed by batch normalization. Next, we flattened the output of the last convolution layer and connected it to the dense block of size five, representing five possible classes. The dense layer used the sigmoid activation function to bring a multi-level approach to the underlying model and develop robust decision-making boundaries (Kang, 2017). Finally, we used the Adam Optimizer with a learning rate of 0.00005; this approach is a common method for optimizing the learning rate of an AI (Kingma & Ba, 2014). The input to the network 2 2 The network used in this study is available on GitHub: https://gitfront.io/r/user-8254535/RsU1BQqLSnwj/Paper-gitfront/. was a single image, and the output of the network was the probability of the different classes. 3.4 Training To train the AI, we reviewed the 9000 images in the entire data set and selected 75 of those images; those images represented the behavior of interest in each of the five modalities (15 images per mode). The images were selected such that they showed each mode at various points across the 5-min video segment. Fig. 2 contains a sample image of a representative behavior for each of the five modalities, including looking off-camera at the computer (Fig. 2a), gesturing a numerical representation (Fig. 2b), imitating the robot's movement (Fig. 2c), looking at workbook (Fig. 2d) and interacting with the robot (Fig. 2e). These images were selected because they fit the description of each mode identified through a priori theory (see Table 1). In order to reduce the complexity of labeling all possible labels for an image of the video frame, the training set was simplified to keep only the most apparent label attributed to it. Specifically, we used a single image per class for validation during training. Using a single image per class reduced the complexities of labeling, which helped prevent the network from overfitting due to the small size of the dataset (Brownlee, 2019). Our training took several trials. In the first trial, the AI did not label all the interactions as expected. To improve the performance of our AI, we increased the number of images in each category to 25 (125 images total) while also refining our image selection so that our images clearly displayed the behavior of interest. While it is generally recommended that 100 or more images are needed to train an AI, researchers have reported success with as few as 25 images when those images are drawn from a larger related data set (see Schouten, Matek, Jacobs, & et al., 2021). In our second trial, the AI yielded 80% accuracy of a match with our manual classification of the training data set. This was achieved by allowing the AI to run the training data for multiple cycles and identifying where the performance dropped off. In our case, it took eight cycles/epochs to achieve 80% accuracy. Across ten different runs, the network was able to achieve 80% accuracy on 7 out of 10 runs. In the remaining three runs, the network obtained a performance of 70% accuracy. We selected the last run in which we obtained 80% accuracy as the model for further tests. The 20% inaccuracy resulted from some images lacking distinct, unique features altogether or a single image resulting in multiple possible categorization outcomes (e.g., one student looks at the computer while the other imitates the robot). 3.5 Testing and validation After training, we tested the AI using the entire set of 9000 images extracted from the 5-min segment of the video. During testing, as with all classification pipelines (Sharma et al., 2018), the network outputs the probability of every possible label. A threshold of 0.5 was chosen to represent a true value for the particular label given an input image; this threshold is recommended as the starting point for model validation when using a training set (Raschka, 2018). This would essentially accept the labels for which the network had at least 50% confidence, which is an example of the default cut-off value of a classifier (Harrison, 2021). While some approaches to validating machine learning recommend iteratively improving the threshold using techniques such as a precision-recall graph, we kept the threshold setting to the default value of 0.5. Our decision was based on the fact that the performance at this threshold was acceptable for the purpose of using the AI to classify images that were later validated through human analysis. To validate the AI results, the AI was configured to automatically annotate the images so that one form of output was a movie consisting of each analyzed image with a display of the interaction type detected in each image. We selected 300 of the 9000 annotated frames, meaning that we selected every 30th frame (i.e., \\u223c1 image per second) so that two of the researchers could determine whether the AI had labeled the images correctly. We chose this rate because humans often view video data on a second-by-second basis when analyzing it for multimodal interactions (Mavers, 2012). This helped establish our process as a human-in-the-loop approach to machine learning in that the contributions of the AI were strongly aligned with the needs and limitations of the human aspects of multimodal analysis (Monarch, 2021). Fig. 3 contains two of the annotated frames produced by the AI. The left image displays an example of an accurate classification by the AI; the girl is looking off-screen to the left at the computer (Com), and the boy is imitating the robot (IR). The right image displays a misclassification in which AI detected an interaction with the computer (Com) that did not occur. The misclassification was largely due to ambiguity regarding whether the boy was looking downward at the workbook (WB) or if he was looking off-screen to the left at the computer (Com). To answer our first question (i.e., How can we develop an AI), we calculated a confusion matrix for each of the five interactions that the AI was trained to identify. A confusion matrix is a tool for understanding the performance of an AI that provides researchers with information about the AI's ability to distinguish one classification from the other classifications (Tiwari, 2022). It is a 2 \\u00d7 2 matrix where each cell contains a value indicating whether the AI-predicted values reflect the presence (i.e., true positive or TP) or absence (i.e., true negative or TN) of a feature in the actual data set. The matrix also notes when an AI detects a feature that is not actually present in the data (i.e., false positive or FP) as well as instances when the AI does not detect a feature that is present (i.e., false negative or FN). The values contained in the cells of the confusion matrix are then used to calculate indicators of performance associated with the AI. As explained by Tiwari (2022), Accuracy reflects the overall amount of correct predictions; it is calculated as the total number of AI/human agreements divided by the total number of evaluations: A c c u r a c y = T P + T N T P + T N + F P + F N Precision reflects how well the model did in making positive predictions compared to all possible positives while Recall reflects how often those positives were accurately predicted: P r e c i s i o n = T P T P + F P R e c a l l = T P T P + F N An F1 value accounts for whether similar levels of precision and recall were achieved: F 1 = 2 \\u00d7 P r e c i s i o n \\u00d7 R e c a l l P r e c i s i o n + R e c a l l Overall, the calculations generated from each confusion matrix provide a robust indication of the performance of the AI because they account for both correct and incorrect predictions made by the AI (Tiwari, 2022). To generate values for our confusion matrices, two of the researchers independently reviewed each of the 300 annotated frames. The goal was to confirm whether the AI had correctly (i.e., True Positive and True Negative) or incorrectly (i.e., False Positive) identified the interaction displayed in the image or whether additional interactions were missing (i.e., False Negative). To ensure the validity of the human analysis, a percent agreement was calculated to establish whether the researchers agreed on the AI's labeling of images. The agreement between reviewers was 95%, suggesting that the two researchers had a very high level of agreement about the AI's classification of images. To answer our second research question (i.e., How can the output support human analysis), we used the true positive rate to establish the frequency with which the participants engaged in each mode of interaction. Because each image reflected 1 s of activity in the video, this frequency analysis provided insight into the modes of interaction that were most dominant. In multimodal analysis, however, the frequency with which a mode occurs may not reflect the importance or meaning of that mode in human reasoning (Bezemer & Jewitt, 2010). To better understand the meaning behind the modes of interaction, we used the AI output to determine where distinct shifts in modes took place (see Fig. 4 a in Section 4). Specifically, we exported the AI classification for each annotated frame into a spreadsheet in which the timing of the image in the sequence (e.g., 0001, 0002) could be paired with the presence of different modes of interaction. This display allowed us to visually inspect the participants' use of modes over time and identify any moments in which those modes shifted; this is important because a shift in modes can reflect a change or development in their thinking and learning (Goffman, 1974). We then used Bezemer's (2014) recommendations for multimodal analysis to analyze the meaning that was reflected through our participants' modes of interaction. Specifically, we generated a multimodal transcript of our participants' interactions at a key moment in which they began using numerical representations as part of their CT. We first paired participant discourse with images of their interactions, including the labels generated by the AI. We then developed larger themes and categories that reflected the meaning behind those labels. This added to the researcher's interpretation of the meaning behind those behaviors in the context of CT. 4 Results To answer our first research question (i.e., How can we develop a supervised machine learning (ML) algorithm), we generated a confusion matrix to evaluate the performance of the AI on each of the five modes of interaction. Table 2 presents the results from the metrics calculated on each confusion matrix. The table displays the instances in which the AI prediction was consistent with human analysis (i.e., True Positive or TP, True Negative or TN), instances when the AI predicted a result that was not present (i.e., False Positive or FP), and instances when the AI did not detect an interaction that was present (i.e., False Negative or FN). Overall, the model showed levels of accuracy greater than 80% for four of the five modes of interaction: Computer, Imitating the Robot, Numerical Representations, and Interacting with the Robot. Accuracy for the Workbook mode was 74%, which was the lowest accuracy across the modes. Precision and Recall were highest for Computer (83% and 93%, respectively) and Numerical Representations (77% and 88%, respectively); this was reflected in the F1 values for Computer and Numerical Representation, which were also the highest at 88% and 82% respectively. The lowest levels of Precision and Recall were found for Imitating the Robot (64% and 47%) and for Interacting with the Robot (100% and 20%); likewise, the F1 values for Imitating the Robot and Interacting with the Robot were lowest at 54% and 33% respectively. The second research question explored how the AI output could be used to support humans in analyzing multimodal video data. To answer this question, we first displayed the modes of interaction identified by the AI in an image-by-image fashion so that shifts in modes could be more easily identified through visual inspection of the AI output. Fig. 4a displays a segment of our spreadsheet in which the image number is noted in the first column, and the corresponding presence of an interaction is noted in the second (e.g., Com = use of computer; Num = Numerical Representation). The True Positive (TP) rate displayed in Table 2 provides a frequency count of the number of times our participants spent engaged in each mode of interaction. Because each image reflects 1 s of our video, the frequencies were converted to time in order to reflect the amount of time spent engaging in each mode. Participants spend the most time interacting with the Computer (3.68 s) and Workbook (1.71 s) and using their hands to make numerical representations (0.83 s). While frequencies are important, multimodal researchers are often more interested in understanding how modes shift over time and the human reasoning behind those shifts (Bezemer, 2014; Mondada, 2018). To identify such shifts, we performed a visual inspection of the modes of interaction displayed in our spreadsheet. Because interactions with the computer (COM) and workbook (WB) were among the most frequent, the visual inspection focused on instances in which other modes of interaction appeared. One evident pattern was that numerical representations would occur in groups - the participants would use them for a period of time, then stop using them for a period of time. To better understand this pattern, we isolated one specific moment in which numerical representations played a key role in our participants' CT. The boy is using four fingers to represent the time setting. At the same time, the girl is explaining the role that the time setting plays in programming the robot: \\u201cEverything is the same besides the time!\\u201d This use of numerical representation suggests that the participants are applying the relationship between distance, speed, and time to their thinking as they program the robot. Because the participants had no prior understanding of this relationship in the context of computer programming, the boy's use of four fingers reveals a critical aspect of the way their CT was associated with their bodily movement. 5 Discussion The primary goal of this study was to contribute to a developing body of literature in which there is a noted need for studies that use a priori theory to guide the development of an AI model that can aid human analysis of learning (see Luckin et al., 2016; Star\\u010di\\u010d, 2019). In terms of our first research question, we designed and developed a theory-driven framework for training the AI to engage in feature detection. We paired a holdout method of validation with human validation of AI output to increase confidence in the validity of the model and the output (Keevers, 2019). We then established the performance of the AI (i.e., our second research question). The results suggest we were successful in many ways. Our validation of the AI output suggests that the AI was able to identify with high levels of precision and recall our participant's interactions with the computer, workbook, and instances in which they used their hands and bodies to represent numbers. The F1 values for the computer and numerical representations, specifically, suggest that the AI was not only able to detect instances of each from non-instances but did so with high levels of precision. These findings support scholars who have suggested that an AI can be trained to engage in feature detection with high levels of accuracy when the approach is grounded in an existing theory and supervised by humans (Luckin & Cukurova, 2019; Sullivan & Keith, 2019). In terms of research on the use of AI in educational research, this type of study is important because it adds to a small but growing interest in using human intervention to train and validate AI to enhance feature detection in small, well-defined data sets (e.g., Nash, Zheng, & Birbilis, 2022; Schouten, Matek, Jacobs, & et al., 2021). The results of this study suggest that a human-in-the-loop approach can be one way to accomplish that goal. The F1 values also indicated that the AI struggled to detect when our participants were imitating the robot and when they were interacting with the robot. In both cases, the recall was low, which suggests that the AI was unable to consistently detect positive instances of each mode. This is likely due to the fact that the AI was trained with 25 images. Similar to our results, Schouten, Matek, Jacobs, & et al., 2021 found 25 images to be enough to train their AI to detect differences among two types of cells. However, the authors also noted how the AI was better able to detect specific regions of pixels when more images were used to train their model. It is likely, then, that the AI in our study had not yet learned to identify the specific regions within our images that indicated when our participants were imitating or interacting with the robot. Imitating the robot can take many forms, from holding one's hands out to turning, twisting, and even moving like the robot. Likewise, the robot often appeared as a black square in the images that lacked any distinct color or boundaries to help the AI distinguish it from other features within the image. This may have made it more difficult for the AI to make a positive prediction about those modes, given the manner in which the AI was trained. Despite the fact that the AI struggled to detect instances of imitating and interacting with the robot, the results are promising overall and provide insight into the ways that AI can be used to address ongoing challenges in multimodal research. One longstanding issue in multimodal analysis of video data deals with establishing validity and inter-rater reliability in one's research. As noted by Bezemer (2014) and others (Mondada, 2018), the strength of any multimodal analysis stems from using techniques that establish consistency among raters and an accurate depiction of the interactions that occurred. Those techniques are often overlooked by researchers because the human analysis of video data is both tedious and time-consuming, which diminishes the potential for multimodal analysis to play a role in better understanding human learning is diminished (Sullivan & Keith, 2019; Worsley, 2018; Worsley et al., 2021). The current study suggests that AI can help address this issue in multimodal research and play an important role in the analysis of multimodal data. In terms of our second research question, the AI served as an additional research team member whose role was to code the video data using five pre-determined codes. The human members of the team ensured agreement across ratings, cross-checking the AI output to ensure accuracy, precision, and validity. Our team then used those ratings to calculate statistics on the performance of our model. Once the performance of the AI was established, we used the output to engage in a detailed analysis of the multimodal video data in ways that went beyond human analysis alone. In our case, we were able to construct an overall idea of the amount of time the participants spent in each mode of interaction. We were also able to identify a key moment in which the AI detected a shift in our participants' use of numerical representations. Transcribing that moment revealed how a numerical gesture reflected a far deeper bodily understanding of the relationship between distance, rate, and time. In the context of educational research, this result is important because it demonstrates how children embody mathematics as part of their CT during educational robotics (see Kopcha et al., 2021). It also adds to a growing body of literature that suggests how children's gestures have been shown to play an important role in the learning of mathematics. Children's gestures can reveal how they are making sense of abstract mathematical concepts and support them in moving to more formal mathematical thinking in the future (Abrahamson & Trninic, 2015; Lakoff & N\\u00fa\\u00f1ez, 2000; Nathan, 2021). Our findings provide needed insight into the ways that AI can reduce the labor-intensive aspects of analyzing children's gestures in video data and support scholars in understanding how mathematics is grounded in one's body (Abrahamson et al., 2021). 5.1 Implications One implication is that AI can be developed through a human-in-the-loop approach to help assuage some of the time-consuming and labor-intensive aspects of multimodal video analysis. The AI produced frames that were annotated with labels indicating which modes of interaction were present. Additionally, those frames can be exported into a spreadsheet in which the timing of the image in the sequence (e.g., 0001, 0002) is paired with the presence of different modes of interaction. This has the potential to save a researcher time when analyzing video data. A researcher can easily use that output to identify key moments in the video data, return to the video, and transcribe both the verbal and non-verbal components of the moment to understand how human learning is taking place. This is important because identifying a shift in modes is an important step in multimodal analysis. A change in the frequency of a mode can indicate how learners are using the environment to support their thinking (see Bieda & Nathan, 2009; Vest et al., 2020). Our study suggests that AI-enhanced analytics can help human researchers better understand what the dominant mode of interaction might be, how it changes over time, and how the frequency of that mode compares to others (see also Andrade et el., 2017; Sullivan & Keith, 2019; Qin et al., 2015). 5.2 Limitations A human-in-the-loop approach is meant to develop a machine-learning model with high levels of human monitoring and intervention to ensure that the model not only has strong performance but serves the needs of the humans who develop it. This typically requires the researcher to make decisions about the development of the model that balance the needs of human with the processes associated with machine learning (Monarch, 2021). For example, our method of validation was more aligned with holdout methods than a k-fold cross-validation technique. We also chose not to analyze the performance of the AI at different thresholds. These decisions may limit how the model can be applied more broadly because they may contribute to overfitting in our model (Raschka, 2018). It is important to note, however, that our goal was not to create an algorithm that could classify images universally. Our goal was to develop a machine learning model that could serve a specific purpose for a limited data set and support humans in multimodal analysis. Our process resulted in a machine learning model that served a variety of practical needs (i.e., lower in cost; greater human involvement) while achieving a level of performance that served the needs of the research team. 5.3 Future research Studying the process of human learning through behaviors is unavoidably time-consuming. Consider that the average 5-min video segment contains \\u223c1000 possible frames/images for analysis. A researcher employing multimodal analysis must first repeatedly review the video to identify the images and dialogue that most clearly communicate the behaviors associated with the phenomenon of interest (Bezemer, 2014; Mondada, 2018). With an AI trained on a robust set of data, the time needed to perform this task can be reduced. Thus, it is not difficult to imagine that the process detailed in this study could be employed across a larger portion of time or video recordings captured across multiple groups. Doing so has the potential to yield thousands of data points upon which to engage in learning analytics and build more sophisticated statistical models that help predict how human behavior relates to learning and cognition. Future research would therefore benefit from establishing how AI-enhanced multimodal analysis might move from smaller to larger segments of video analysis and increase the amounts of data extracted from a video data set. This contributes to the current call for methods that allow researchers to study the social aspects of learning through large sets of multimodal data (Andrade et al., 2017). This study also offers some implications for other scholars who are interested in employing AI as a tool for enhancing their own multimodal analysis of video data. In the training of our AI, we sought images that displayed a single mode at one time. While this did work in our study, it may have contributed to errors in the AI's ability to detect our participants imitating and interacting with the robot. Future research could explore how training sets of different sizes might affect the AI (similar to Schouten, Matek, Jacobs, & et al., 2021) and how techniques such as heatmap generation could improve the AI's sensitivity to multiple modes being present at the same time (see Bach et al., 2015). 6 Conclusion Many papers note the potential for supervised approaches to AI to support educational research; few, however, articulate the process in which those approaches can be achieved (Andrade-Lotero & Danish, 2014; Spikol et al., 2018). This paper provides a detailed summary of one such process in which we used a supervised, human-in-the-loop approach to explore the role of AI in multimodal analysis. While this is a relatively new frontier in educational research, the results of this study suggest that an AI algorithm can be built on learning theory to detect human behavior in video data and help a researcher better understand learning through the temporal and spatial organization of that behavior. We hope that this paper provides insight for others who aim to explore how AI can meaningfully inform the human interpretation of multimodal data. Funding This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors. Ethics statement There were no ethical issues related to the selection and treatment of subjects associated with this paper Data availability statement The data can be accessed by contacting the first author. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. References Abrahamson and Trninic, 2015 D. Abrahamson D. Trninic Bringing forth mathematical concepts: Signifying sensorimotor enactment in fields of promoted action ZDM Mathematics Education 47 2 2015 1 12 10.1007/s11858-014-0620-0 Abrahamson, D., & Trninic, D. (2015). Bringing forth mathematical concepts: signifying sensorimotor enactment in fields of promoted action. ZDM Mathematics Education, 47(2), 1-12. https://doi.org/10.1007/s11858-014-0620-0 Abrahamson et al., 2021 D. Abrahamson M. Worsley Z.A. Pardos L. Ou Learning analytics of embodied design: Enhancing synergy International Journal of Child-Computer Interaction 32 2022 2021 1 5 10.1016/j.ijcci.2021.100409 Abrahamson, D., Worsley, M., Pardos, Z. A., & Ou, L. (2021). Learning analytics of embodied design: Enhancing synergy. International Journal of Child-Computer Interaction, 32(2022), 1-5. https://doi.org/10.1016/j.ijcci.2021.100409 Andrade-Lotero and Danish, 2014 A. Andrade-Lotero J.A. Danish Advancing epistemological frame analysis to refine our understanding of inquiry frames in early elementary interviews J.L. Polman E.A. Kyza D.K. O'Neill I. Tabak W.R. Penuel A.S. Jurow K. O'Connor T. Lee L. D'Amico Learning and becoming in practice: The international conference of the learning sciences (ICLS) 2014 2 2014 International Society of the Learning Sciences Boulder, CO 10.22318/icls2014.1637 Andrade-Lotero, A. & Danish, J. A. (2014). Advancing epistemological frame analysis to refine our understanding of inquiry frames in early elementary interviews. In Polman, J. L., Kyza, E. A., O'Neill, D. K., Tabak, I., Penuel, W. R., Jurow, A. S., O'Connor, K., Lee, T., and D'Amico, L. (Eds.). (2014). Learning and becoming in practice: The International Conference of the Learning Sciences (ICLS) 2014, Volume 2. Boulder, CO: International Society of the Learning Sciences.https://doi.org/10.22318/icls2014.1637 Andrade et al., 2017 A. Andrade J.A. Danish A.V. Maltese A measurement model of gestures in an embodied learning environment: Accounting for temporal dependencies Journal of learning Analytics 4 3 2017 18 46 10.18608/jla.2017.43.3 Andrade, A., Danish, J. A., & Maltese, A. V. (2017). A measurement model of gestures in an embodied learning environment: Accounting for temporal dependencies. Journal of learning Analytics, 4(3), 18-46. http://dx.doi.org/10.18608/jla.2017.43.3 Andrade et al., 2016 A. Andrade G. Delandshere J.A. Danish Using multimodal learning analytics to model student behaviour: A systematic analysis of behavioural framing Journal of Learning Analytics 3 2 2016 282 306 10.18608/jla.2016.32.14 Andrade, A., Delandshere, G., & Danish, J. A. (2016). Using multimodal learning analytics to model student behaviour: A systematic analysis of behavioural framing. Journal of Learning Analytics, 3(2), 282-306. http://dx.doi.org/10.18608/jla.2016.32.14 Bach et al., 2015 S. Bach A. Binder G. Montavon F. Klauschen K.R. M\\u00fcller W. Samek On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation PLoS One 10 7 2015 1 30 10.1371/journal.pone.0130140 Bach, S., Binder, A., Montavon, G., Klauschen, F., Muller, K. R., & Samek, W. (2015). On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.PloS one, 10(7), 1-30. https://doi.org/10.1371/journal.pone.0130140 Bakala et al., 2021 E. Bakala A. Gerosa J.P. Hourcade G. Tejera Preschool children, robots, and computational thinking: A systematic review International Journal of Child-Computer Interaction 29 2021 1 20 10.1016/j.ijcci.2021.100337 Bakala, E., Gerosa, A., Hourcade, J. P., & Tejera, G. (2021). Preschool children, robots, and computational thinking: A systematic review. International Journal of Child-Computer Interaction, 29(2021), 1-20. https://doi.org/10.1016/j.ijcci.2021.100337 Bezemer, 2014 J. Bezemer Multimodal transcription: A case study Interactions, images and texts: A reader in multimodality 11 2014 155 Bezemer, J. (2014). Multimodal transcription: A case study. Interactions, images and texts: A reader in multimodality, 11, 155. Bezemer and Jewitt, 2010 J. Bezemer C. Jewitt Multimodal analysis: Key issues L. Litosseliti Research methods in linguistics 2010 Continuum London, UK 180 197 Bezemer, J., & Jewitt, C. (2010). Multimodal analysis: Key issues. In L. Litosseliti (Ed.), Research methods in linguistics (pp. 180-197). London, UK: Continuum Bieda and Nathan, 2009 K.N. Bieda M.J. Nathan Representational disfluency in algebra: Evidence from student gestures and speech ZDM 41 5 2009 637 650 10.1007/s11858-009-0198-0 Bieda, K. N., & Nathan, M. J. (2009). Representational disfluency in algebra: Evidence from student gestures and speech. ZDM, 41(5), 637-650. http://dx.doi.org/10.1007/s11858-009-0198-0 Brownlee, 2019 J. Brownlee How to avoid overfitting in deep learning neural networks. Machine Learning Mastery 2019 Retrieved June 4, 2023, from https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/ Brownlee, J. (2019, August 6). How to avoid overfitting in deep learning neural networks. Machine Learning Mastery. Retrieved June 4, 2023, from https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/ Chen et al., 2017 G. Chen J. Shen L. Barth-Cohen S. Jiang X. Huang M. Eltoukhy Assessing elementary students' computational thinking in everyday reasoning and robotics programming Computers & Education 109 2017 162 175 10.1016/j.compedu.2017.03.001 Chen, G., Shen, J., Barth-Cohen, L., Jiang, S., Huang, X., & Eltoukhy, M. (2017). Assessing elementary students\\u2019 computational thinking in everyday reasoning and robotics programming. Computers & Education, 109, 162-175. https://doi.org/10.1016/j.compedu.2017.03.001 Chen et al., 2020 X. Chen H. Xie D. Zou G.J. Hwang Application and theory gaps during the rise of artificial intelligence in education Computers in Education: Artificial Intelligence 1 2020 100002 10.1016/j.caeai.2020.100002 Chen, X., Xie, H., Zou, D., & Hwang, G. J. (2020). Application and theory gaps during the rise of artificial intelligence in education. Computers and Education: Artificial Intelligence, 1, 100002. https://doi.org/10.1016/j.caeai.2020.100002 Crescenzi\\u2010Lanna, 2020 L. Crescenzi\\u2010Lanna Multimodal learning analytics research with young children: A systematic review British Journal of Educational Technology 51 5 2020 1485 1504 10.1111/bjet.12959 Crescenzi-Lanna, L. (2020). Multimodal Learning Analytics research with young children: A systematic review. British Journal of Educational Technology, 51(5), 1485-1504. https://doi.org/10.1111/bjet.12959 Cukurova et al., 2019 M. Cukurova C. Kent R. Luckin Artificial intelligence and multimodal data in the service of human decision-making: A case study in debate tutoring British Journal of Educational Technology 50 6 2019 3032 3046 10.1111/bjet.12829 Cukurova, M., Kent, C., & Luckin, R. (2019). Artificial intelligence and multimodal data in the service of human decision-making: A case study in debate tutoring. British Journal of Educational Technology, 50(6), 3032-3046. https://doi.org/10.1111/bjet.12829 Cukurova et al., 2019 M. Cukurova R. Luckin A. Clark-Wilson Creating the golden triangle of evidence-informed education technology with EDUCATE British Journal of Educational Technology 50 2 2019 490 504 10.1111/bjet.12727 Cukurova, M., Luckin, R., & Clark-Wilson, A. (2019). Creating the golden triangle of evidence-informed education technology with EDUCATE. British Journal of Educational Technology, 50(2), 490-504. https://doi.org/10.1111/bjet.12727 Cukurova et al., 2018 M. Cukurova R. Luckin E. Mill\\u00e1n M. Mavrikis The NISPI framework: Analysing collaborative problem-solving from students' physical interactions Computers & Education 116 2018 93 109 10.1016/j.compedu.2017.08.007 Cukurova, M., Luckin, R., Millan, E., & Mavrikis, M. (2018). The NISPI framework: Analysing collaborative problem-solving from students' physical interactions. Computers & Education, 116, 93-109. https://doi.org/10.1016/j.compedu.2017.08.007 Di Mitri et al., 2017 D. Di Mitri M. Scheffel H. Drachsler D. B\\u00f6rner S. Ternier M. Specht Learning pulse: A machine learning approach for predicting performance in self-regulated learning using multimodal data Proceedings of the seventh international learning analytics & knowledge conference 2017 188 197 10.1145/3027385.3027447 Di Mitri, D., Scheffel, M., Drachsler, H., Borner, D., Ternier, S., & Specht, M. (2017). Learning pulse: A machine learning approach for predicting performance in self-regulated learning using multimodal data. In Proceedings of the Seventh International Learning Analytics & Knowledge Conference, 188-197. https://doi.org/10.1145/3027385.3027447 Feuston and Brubaker, 2021 J.L. Feuston J.R. Brubaker Putting tools in their place: The role of time and perspective in human-AI collaboration for qualitative analysis Proceedings of the ACM on human-computer interaction, 5(CSCW2) 2021 1 25 10.1145/3479856 Feuston, J. L., & Brubaker, J. R. (2021). Putting tools in their place: The role of time and perspective in human-AI collaboration for qualitative analysis. In Proceedings of the ACM on Human-Computer Interaction, 5(CSCW2), 1-25. https://doi.org/10.1145/3479856 Goffman, 1974 E. Goffman Frame analysis: An essay on the organization of experience 1974 Harvard University Press Goffman, E. (1974). Frame analysis: An essay on the organization of experience. Harvard University Press. Harrison, 2021 G. Harrison Calculating and setting thresholds to optimise logistic regression performance Data Science 2021 Retrieved April 24, 2023, from https://towardsdatascience.com/calculating-and-setting-thresholds-to-optimise-logistic-regression-performance-c77e6d112d7e Harrison, G. (2021, May 2). Calculating and setting thresholds to optimise logistic regression performance. Toward Data Science. Retrieved April 24, 2023, from https://towardsdatascience.com/calculating-and-setting-thresholds-to-optimise-logistic-regression-performance-c77e6d112d7e Jiang et al., 2021 J.A. Jiang K. Wade C. Fiesler J.R. Brubaker Supporting serendipity: Opportunities and challenges for Human-AI Collaboration in qualitative analysis Proceedings of the ACM on Human-Computer Interaction 5 CSCW1 2021 1 23 10.1145/3449168 Jiang, J. A., Wade, K., Fiesler, C., & Brubaker, J. R. (2021). Supporting serendipity: Opportunities and challenges for Human-AI Collaboration in qualitative analysis. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1-23. https://doi.org/10.1145/3449168 Kang, 2017 N. Kang Multi-layer neural Networks with sigmoid function\\u2014 deep Learning for rookies (2). Towards data science 2017 Retrieved June 4, 2023, from https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f Kang, N. (2017, June 27). Multi-Layer Neural Networks with Sigmoid Function- Deep Learning for Rookies (2). Towards Data Science. Retrieved June 4, 2023, from https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f Keevers, 2019 T.L. Keevers Cross-validation is insufficient for model validation Joint and operations analysis division 2019 Defence Science and Technology Group Victoria, Australia Keevers, T. L. (2019). Cross-validation is insufficient for model validation. Joint and Operations Analysis Division, Defence Science and Technology Group: Victoria, Australia Kendon, 1982 A. Kendon Conducting interaction: Patterns of behavior in focused encounters 1982 Cambridge University Press Cambridge, England Kendon, A. (1982). Conducting interaction: Patterns of behavior in focused encounters. Cambridge, England: Cambridge University Press. Kingma and Ba, 2014 D.P. Kingma J. Ba Adam: A method for stochastic optimization Proceedings of the 3rd international conference for learning representations 2014 10.48550/arXiv.1412.6980 Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference for Learning Representations. https://doi.org/10.48550/arXiv.1412.6980 Kopcha et al., 2017 T.J. Kopcha J. McGregor S. Shin Y. Qian J. Choi R. Hill I. Choi Developing an integrative STEM curriculum for robotics education through Educational Design Research Journal of Formative Design in Learning 1 1 2017 31 44 10.1007/s41686-017-0005-1 Kopcha, T. J., McGregor, J., Shin, S., Qian, Y., Choi, J., Hill, R., ... & Choi, I. (2017).Developing an integrative STEM curriculum for robotics education through Educational Design Research. Journal of Formative Design in Learning, 1(1), 31-44. https://doi.org/10.1007/s41686-017-0005-1. Kopcha and Ocak, 2019 T. Kopcha C. Ocak Embodiment of computational thinking during collaborative robotics activity K. Lund G.P. Niccolai E. Lavou\\u00e9 C. Hmelo-Silver G. Gweon M. Baker A wide lens: Combining embodied, enactive, extended, and embedded learning in collaborative settings 13th international conference on computer supported collaborative learning (CSCL) 2019 1 2019 International Society of the Learning Sciences Lyon, France 464 471 https://repository.isls.org//handle/1/1604 Kopcha, T. & Ocak, C. (2019). Embodiment of Computational Thinking During Collaborative Robotics Activity. In Lund, K., Niccolai, G. P., Lavoue, E., Hmelo-Silver, C., Gweon, G., & Baker, M. (Eds.), A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings, 13th International Conference on Computer Supported Collaborative Learning (CSCL) 2019, Volume 1 (pp. 464-471). Lyon, France: International Society of the Learning Sciences. https://repository.isls.org//handle/1/1604 Kopcha et al., 2021 T.J. Kopcha C. Ocak Y. Qian Analyzing children's computational thinking through embodied interaction with technology: A multimodal perspective Educational Technology Research & Development 69 2021 1987 2012 10.1007/s11423-020-09832-y Kopcha, T. J., Ocak, C., & Qian, Y. (2021). Analyzing children\\u2019s computational thinking through embodied interaction with technology: A multimodal perspective. Educational Technology Research and Development, 69, 1987-2012. https://doi.org/10.1007/s11423-020-09832-y Lakoff and N\\u00fa\\u00f1ez, 2000 G. Lakoff R.E. N\\u00fa\\u00f1ez Where mathematics comes from: How the embodied mind brings mathematics into being 2000 Basic Books New York, NY Lakoff, G., & Nunez, R. E. (2000). Where mathematics comes from: How the embodied mind brings mathematics into being. New York, NY: Basic Books LeCun and Bengio, 1995 Y. LeCun Y. Bengio Convolutional networks for images, speech, and time series The handbook of brain theory and neural networks 3361 1995 1995 10 LeCun, Y., & Bengio, Y. (1995). Convolutional networks for images, speech, and time series. In The handbook of brain theory and neural networks, 3361(10), 1995. Lee-Cultura et al., 2022 S. Lee-Cultura K. Sharma M. Giannakos Children's play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach International Journal of Child-Computer Interaction 31 2022 100355 10.1016/j.ijcci.2021.100355 Lee-Cultura, S., Sharma, K., & Giannakos, M. (2022). Children\\u2019s play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach. International Journal of Child-Computer Interaction, 31, 100355. https://doi.org/10.1016/j.ijcci.2021.100355 Luckin and Cukurova, 2019 R. Luckin M. Cukurova Designing educational technologies in the age of AI: A learning sciences-driven approach British Journal of Educational Technology 50 6 2019 2824 2838 10.1111/bjet.12861 Luckin, R., & Cukurova, M. (2019). Designing educational technologies in the age of AI: A learning sciences-driven approach. British Journal of Educational Technology, 50(6), 2824-2838. https://doi.org/10.1111/bjet.12861 Luckin, Holmes, Griffiths, &#38; Forcier, 2016 R. Luckin W. Holmes M. Griffiths L.B. Forcier Intelligence Unleashed 2016 An argument for AI in Education London Luckin, R., Holmes, W., Griffiths, M., & Forcier, L. B. (2016). Intelligence Unleashed. An argument for AI in Education. London: Pearson. Marathe and Toyama, 2018 M. Marathe K. Toyama Semi-Automated coding for qualitative research, A user-centered inquiry and initial prototypes CHI \\u201918, proceedings of the 2018 CHI conference on human factors in computing systems 2018 10.1145/3173574.3173922 Marathe, M., K. Toyama 2018. Semi-Automated Coding for Qualitative Research, A User-Centered Inquiry and Initial Prototypes. In CHI \\u201918, Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3173574.3173922. Martin and Sherin, 2013 T. Martin B. Sherin Learning analytics and computational techniques for detecting and evaluating patterns in learning: An introduction to the special issue The Journal of the Learning Sciences 22 4 2013 511 520 10.1080/10508406.2013.840466 Martin, T., & Sherin, B. (2013). Learning analytics and computational techniques for detecting and evaluating patterns in learning: An introduction to the special issue. Journal of the Learning Sciences, 22(4), 511-520. https://doi.org/10.1080/10508406.2013.840466 Mavers, 2012 Mavers, D. (2012). Transcribing video. National Centre for Research Methods. Retrieved June 14, 2023, from https://eprints.ncrm.ac.uk/id/eprint/2877/4/NCRM_working_paper0512.pdf. Monarch, 2021 R.M. Monarch Human-in-the-Loop machine learning: Active learning and annotation for human-centered AI 2021 Simon and Schuster Monarch, R. M. (2021). Human-in-the-Loop Machine Learning: Active learning and annotation for human-centered AI. Simon and Schuster. Mondada, 2018 L. Mondada Multiple temporalities of language and body in interaction: Challenges for transcribing multimodality Research on Language and Social Interaction 51 1 2018 85 106 10.1080/08351813.2018.1413878 Mondada, L. (2018). Multiple temporalities of language and body in interaction: challenges for transcribing multimodality. Research on Language and Social Interaction, 51(1), 85-106. https://doi.org/10.1080/08351813.2018.1413878 Mosqueira-Rey et al., 2023 E. Mosqueira-Rey E. Hern\\u00e1ndez-Pereira D. Alonso-R\\u00edos J. Bobes-Bascar\\u00e1n \\u00c1. Fern\\u00e1ndez-Leal Human-in-the-loop machine learning: A state of the art Artificial Intelligence Review 56 4 2023 3005 3054 Mosqueira-Rey, E., Hernandez-Pereira, E., Alonso-Rios, D., Bobes-Bascaran, J., & Fernandez-Leal, A. (2023). Human-in-the-loop machine learning: A state of the art. Artificial Intelligence Review, 56(4), 3005-3054. Nash et al., 2022 W. Nash L. Zheng N. Birbilis Deep learning corrosion detection with confidence Npj Materials Degradation 6 1 2022 26 10.1038/s41529-022-00232-6 Nash, W., Zheng, L., & Birbilis, N. (2022). Deep learning corrosion detection with confidence.npj Materials Degradation, 6(1), 26. https://doi.org/10.1038/s41529-022-00232-6 Nathan, 2021 M.J. Nathan Foundations of embodied learning: A paradigm for education 2021 Routledge Nathan, M. J. (2021). Foundations of embodied learning: A paradigm for education. Routledge Ocak et al., 2021 C. Ocak T.J. Kopcha R. Dey An AI-enhanced pattern recognition approach to analyze children's embodied interactions M.M.T. Rodrigo Proceedings of the 29th international conference on computers in education. Asia-pacific society for computers in education 2021 273 278 Ocak, C., Kopcha, T. J., & Dey, R. (2021). An AI-enhanced Pattern Recognition Approach to Analyze Children\\u2019s Embodied Interactions. In Rodrigo, M. M. T. et al. (Eds.) (2021). Proceedings of the 29th International Conference on Computers in Education. Asia-Pacific Society for Computers in Education, pp. 273 - 278 Qin et al., 2015 J. Qin Y. Zhou H. Lu H. Ya Teaching video analytics based on student spatial and temporal behavior mining \\u201915. Proceedings of the 5th ACM on international conference on multimedia retrieval - ICMR 2015 635 642 10.1145/2671188.2749357 Qin, J., Zhou, Y., Lu, H., & Ya, H. (2015). Teaching video analytics based on student spatial and temporal behavior mining. In, \\u201915. Proceedings of the 5th ACM on international conference on multimedia retrieval - ICMR (pp. 635-642). https://doi.org/10.1145/2671188.2749357 Ramakrishnan et al., 2019, A. Ramakrishnan E. Ottmar J. LoCasale-Crouch J. Whitehill Toward automated classroom observation: Predicting positive and negative climate 2019 14th IEEE international conference on automatic face & gesture recognition (FG 2019) 2019 IEEE 1 8 Ramakrishnan, A., Ottmar, E., LoCasale-Crouch, J., & Whitehill, J. (2019, May). Toward automated classroom observation: Predicting positive and negative climate. In 2019 14th IEEE international conference on automatic face & gesture recognition (FG 2019) (pp. 1-8). IEEE. Raschka, 2018 S. Raschka Model evaluation, model selection, and algorithm selection in machine learning 2018 10.48550/arXiv.1811.12808 Raschka, S. (2018). Model evaluation, model selection, and algorithm selection in machine learning. https://doi.org/10.48550/arXiv.1811.12808 Reilly et al., 2018 J.M. Reilly M. Ravenell B. Schneider Exploring collaboration using motion sensors and multi-modal learning analytics K.E. Boyer M. Yudelson International educational data mining society, 11th international conference on educational data mining (EDM) 2018 2018 International Conference on Educational Data Mining Buffalo, NY 333 339 Reilly, J. M., Ravenell, M., & Schneider, B. (2018). Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics. In Boyer, K. E., & Yudelson, M. (Eds.), International Educational Data Mining Society, 11th International Conference on Educational Data Mining (EDM) 2018, (pp. 333 - 339 ). Buffalo, NY: International Conference on Educational Data Mining. Schouten et al., 2021 J.P.E. Schouten C. Matek L.F.P. Jacobs Tens of images can suffice to train neural networks for malignant leukocyte detection Scientific Reports 11 2021 7995 10.1038/s41598-021-86995-5 Schouten, J.P.E., Matek, C., Jacobs, L.F.P. et al. (2021). Tens of images can suffice to train neural networks for malignant leukocyte detection. Sci Rep 11, 7995. https://doi.org/10.1038/s41598-021-86995-5 Sharma et al., 2018 N. Sharma V. Jain A. Mishra An analysis of convolutional neural networks for image classification Procedia Computer Science 132 2018 377 384 10.1016/j.procs.2018.05.198 Sharma, N., Jain, V., & Mishra, A. (2018). An analysis of convolutional neural networks for image classification. Procedia Computer Science, 132, 377-384. http://doi.org/10.1016/j.procs.2018.05.198 Sharma et al., 2019 K. Sharma Z. Papamitsiou M. Giannakos Building pipelines for educational data using AI and multimodal analytics: A \\u201cgrey-box\\u201d approach British Journal of Educational Technology 50 6 2019 3004 3031 10.1111/bjet.12854 Sharma, K., Papamitsiou, Z., & Giannakos, M. (2019). Building pipelines for educational data using AI and multimodal analytics: A \\u201cgrey-box\\u201d approach. British Journal of Educational Technology, 50(6), 3004-3031. http://doi.org/10.1111/bjet.12854 Spikol et al., 2017 D. Spikol E. Ruffaldi M. Cukurova Using multimodal learning analytics to identify aspects of collaboration in project-based learning B.K. Smith M. Borge E. Mercier K.Y. Lim (2017). Making a difference: Prioritizing equity and access in CSCL, 12th international conference on computer supported collaborative learning (CSCL) 2017 1 2017 International Society of the Learning Sciences Philadelphia, PA https://repository.isls.org//handle/1/240 Spikol, D., Ruffaldi, E., & Cukurova, M. (2017). Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning In Smith, B. K., Borge, M., Mercier, E., and Lim, K. Y. (Eds.). (2017). Making a Difference: Prioritizing Equity and Access in CSCL, 12th International Conference on Computer Supported Collaborative Learning (CSCL) 2017, Volume 1. Philadelphia, PA: International Society of the Learning Sciences. https://repository.isls.org//handle/1/240 Spikol et al., 2018 D. Spikol E. Ruffaldi G. Dabisias M. Cukurova Supervised machine learning in multimodal learning analytics for estimating success in project-based learning Journal of Computer Assisted Learning 34 4 2018 366 377 10.1111/jcal.12263 Spikol, D., Ruffaldi, E., Dabisias, G., & Cukurova, M. (2018). Supervised machine learning in multimodal learning analytics for estimating success in project-based learning. Journal of Computer Assisted Learning, 34(4), 366-377. http://doi.org/10.1111/jcal.12263 Star\\u010di\\u010d, 2019 A.I. Star\\u010di\\u010d Human learning and learning analytics in the age of artificial intelligence British Journal of Educational Technology 50 6 2019 2974 2976 10.1111/bjet.12879 Starcic, A. I. (2019). Human learning and learning analytics in the age of artificial intelligence. British Journal of Educational Technology, 50(6), 2974-2976. http://doi.org/10.1111/bjet.12879 Sullivan and Keith, 2019 F.R. Sullivan P.K. Keith Exploring the potential of natural language processing to support microgenetic analysis of collaborative learning discussions British Journal of Educational Technology 50 6 2019 3047 3063 10.1111/bjet.12875 Sullivan, F. R., & Keith, P. K. (2019). Exploring the potential of natural language processing to support microgenetic analysis of collaborative learning discussions. British Journal of Educational Technology, 50(6), 3047-3063. http://doi.org/10.1111/bjet.12875 Su and Yang, 2023 J. Su W. Yang A systematic review of integrating computational thinking in early childhood education Computers and Education Open 2023 100122 10.1016/j.caeo.2023.100122 Su, J., & Yang, W. (2023). A systematic review of integrating computational thinking in early childhood education. Computers and Education Open, 100122. https://doi.org/10.1016/j.caeo.2023.100122 Tang et al., 2020 X. Tang Y. Yin Q. Lin R. Hadad X. Zhai Assessing computational thinking: A systematic review of empirical studies Computers & Education 148 2020 103798 10.1016/j.compedu.2019.103798 Tang, X., Yin, Y., Lin, Q., Hadad, R., & Zhai, X. (2020). Assessing computational thinking: A systematic review of empirical studies. Computers & Education, 148, 103798. https://doi.org/10.1016/j.compedu.2019.103798 Tiwari, 2022 A. Tiwari Supervised learning: From theory to applications R. Pandey S.K. Khatri N.K. Singh P. Verma Artificial intelligence and machine learning for EDGE computing 2022 Academic Press 10.1016/B978-0-12-824054-0.00026-5 Tiwari, A. (2022). Supervised learning: From theory to applications. In Pandey, R., Khatri, S. K., Singh, N. K., & Verma, P. (Eds.), Artificial intelligence and machine learning for EDGE computing. Academic Press. https://doi.org/10.1016/B978-0-12-824054-0.00026-5 Vest et al., 2020 N.A. Vest E.R. Fyfe M.J. Nathan M.W. Alibali Learning from an avatar video instructor: The role of gesture mimicry Gesture 19 1 2020 128 155 10.1075/gest.18019.ves Vest, N. A., Fyfe, E. R., Nathan, M. J., & Alibali, M. W. (2020). Learning from an avatar video instructor: The role of gesture mimicry. Gesture, 19(1), 128-155. https://doi.org/10.1075/gest.18019.ves Worsley, 2018 M. Worsley Multimodal learning analytics' past, present, and potential futures Companion proceedings of the eighth international conference on learning analytics and knowledge (LAK 2018), 5\\u20139 march 2018, sydney, Australia 2018 SoLAR http://bit.ly/lak18-companion-proceedings Worsley, M. (2018). Multimodal learning analytics\\u2019 past, present, and potential futures. In Companion Proceedings of the Eighth International Conference on Learning Analytics and Knowledge (LAK 2018), 5-9 March 2018, Sydney, Australia. SoLAR. http://bit.ly/lak18-companion-proceedings Worsley and Blikstein, 2018 M. Worsley P. Blikstein A multimodal analysis of making International Journal of Artificial Intelligence in Education 28 3 2018 385 419 10.1007/s40593-017-0160-1 Worsley, M., & Blikstein, P. (2018). A multimodal analysis of making. International Journal of Artificial Intelligence in Education, 28(3), 385-419. https://doi.org/10.1007/s40593-017-0160-1 Worsley et al., 2021 M. Worsley R. Martinez-Maldonado C. D'Angelo A new era in multimodal learning analytics: Twelve core commitments to ground and grow MMLA Journal of Learning Analytics 8 3 2021 10 27 10.18608/jla.2021.7361 Worsley, M., Martinez-Maldonado, R., & D'Angelo, C. (2021). A new era in multimodal learning analytics: twelve core commitments to ground and grow MMLA. Journal of Learning Analytics, 8(3), 10-27. https://doi.org/10.18608/jla.2021.7361 Zhang and Nouri, 2019 L. Zhang J. Nouri A systematic review of learning computational thinking through Scratch in K-9 Computers & Education 141 2019 103607 10.1016/j.compedu.2019.103607 Zhang, L., & Nouri, J. (2019).A systematic review of learning computational thinking through Scratch in K-9. Computers & Education, 141, 103607. https://doi.org/10.1016/j.compedu.2019.103607 Glossary CT Computational thinking AI Artificial intelligence Com Use of computer Num Numerical Representation WB Workbook IR Imitating the robot Rob Robot Ceren Ocak, Ph.D., is an Assistant Professor of Instructional Technology in the Department of Leadership, Technology, And Human Development at Georgia Southern University. Her research focuses on computer science education in K-12 and embodied learning. Theodore J. Kopcha, Ph.D., is an Associate Professor of Learning, Design, and Technology in the Department of Career and Information Studies at the University of Georgia. His research focuses on embodied perspectives in K-12 learning in STEM and beyond. Raunak Dey, Ph.D., received his Ph.D. from the Department of Computer Science at the University of Georgia. He is researching AI and Genomics. His area of interest is primarily Artificial Intelligence, Heuristics, and Genomics.\"}]\n"
     ]
    }
   ],
   "source": [
    "# print(json.dumps(pii_doc.data[\"originalText\"]))\n",
    "\n",
    "# print(pii_doc.title)\n",
    "text = pii_doc.data[\"originalText\"]\n",
    "\n",
    "# Extract the relevant portion of the string\n",
    "pattern = r\"Published by Elsevier Ltd\\.(.*?)Reference(.*)\"\n",
    "match = re.search(pattern, text, re.DOTALL)\n",
    "section_info = match.group(1).strip()\n",
    "full_text = match.group(2).strip()\n",
    "\n",
    "\n",
    "# Extract the section names\n",
    "pattern = r\"\\b(\\d{1,2}(?:\\.\\d{1,2})?)\\s+([A-Za-z\\s:#]+?(?=\\s\\d|$))\"\n",
    "matches = re.findall(pattern, section_info)\n",
    "\n",
    "sections = []\n",
    "for section_number, section_name in matches:\n",
    "    sections.append(f\"{section_number} {section_name.strip()}\")\n",
    "\n",
    "# Use the section names to get the section contents\n",
    "section_index = [full_text.find(section) for section in sections]\n",
    "section_contents = []\n",
    "\n",
    "i = 0\n",
    "while i < len(sections)-1:\n",
    "    section_contents.append(full_text[section_index[i] + len(sections[i]) : section_index[i+1] + len(sections[i+1])])\n",
    "    i+=1\n",
    "\n",
    "section_contents.append(full_text[section_index[i] + len(sections[i]) :])\n",
    "\n",
    "result = [{\"section\": n, \"content\": c} for n, c  in zip(sections, section_contents)]\n",
    "print(json.dumps(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 Introduction'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][\"section\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "Some text before\n",
    "Published by Elsevier Ltd.\n",
    "This is the content you want to extract.\n",
    "Published by Elsevier Ltd.\n",
    "Another instance of the pattern.\n",
    "\"\"\"\n",
    "\n",
    "# Define the pattern\n",
    "pattern = r\"Published by Elsevier Ltd\\.\"\n",
    "\n",
    "# Find all matches using re.finditer()\n",
    "matches = re.finditer(pattern, text)\n",
    "\n",
    "# Count the number of matches\n",
    "num_matches = sum(1 for _ in matches)\n",
    "\n",
    "print(\"Number of matches:\", num_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published by Elsevier Ltd.\\nThis is the content you want to extract.\\nReference'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "Some text before\n",
    "Published by Elsevier Ltd.\n",
    "This is the content you want to extract.\n",
    "Reference\n",
    "Some text after\n",
    "\"\"\"\n",
    "\n",
    "# Define the pattern\n",
    "pattern = r\"Published by Elsevier Ltd\\.(.*?)Reference\"\n",
    "\n",
    "# Find the match using regular expression\n",
    "section_info = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "section_info.group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the introduction section.\n",
      "hi\n",
      "\n",
      "This is the investigative research section.\n",
      "hi\n",
      "\n",
      "Hi\n",
      "hi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example full text\n",
    "full_text = \"\"\"\n",
    "JOHNSON P 1 Introduction\n",
    "This is the introduction section.\n",
    "2 Background\n",
    "This is the background section.\n",
    "3 Investigative research\n",
    "This is the investigative research section.\n",
    "4 Results and analysis\n",
    "4.1 Magic act # 1: Hidden in plain sight: When partnerships are not partnerships\n",
    "This is the first subsection of the Results and analysis section.\n",
    "4.2 Magic act #2: The disappearing act\n",
    "This is the second subsection of the Results and analysis section.\n",
    "5 Conclusion\n",
    "Hi\n",
    "\"\"\"\n",
    "\n",
    "# Example list of section names\n",
    "section_names = ['1 Introduction', '2 Background', '3 Investigative research', '4 Results and analysis', '5 Conclusion']\n",
    "\n",
    "# Regular expression pattern for matching section headers\n",
    "pattern = r'\\b(\\d+\\.\\s*)?(' + '|'.join(re.escape(name) for name in section_names) + r')\\b'\n",
    "\n",
    "# Find section contents using regular expressions\n",
    "section_contents = []\n",
    "matches = re.finditer(pattern, full_text, re.IGNORECASE)\n",
    "for section_info in matches:\n",
    "    section_start = section_info.end()\n",
    "    if section_info.group(1):  # If there is a subsection number\n",
    "        section_start += len(section_info.group(1))\n",
    "    next_match = next(matches, None)\n",
    "    section_end = next_match.start() if next_match else None\n",
    "    section_content = full_text[section_start:section_end].strip() if section_end else full_text[section_start:].strip()\n",
    "    section_contents.append(section_content)\n",
    "\n",
    "# Print the section contents\n",
    "for content in section_contents:\n",
    "    print(content)\n",
    "    print(\"hi\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "1 Introduction\n",
      "This is the introduction section.\n",
      "------------------------------\n",
      "2 Background: Contextualising the Newcastle 500 from a political economy perspective\n",
      "2 Background: Contextualising the Newcastle 500 from a political economy perspective\n",
      "This is the background section.\n",
      "------------------------------\n",
      "3 Investigative research\n",
      "3 Investigative research\n",
      "This is the investigative research section\n",
      "------------------------------\n",
      "4 Conclusion\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "JOHNSON P 1 Introduction\n",
    "This is the introduction section.\n",
    "2 Background: Contextualising the Newcastle 500 from a political economy perspective\n",
    "This is the background section.\n",
    "3 Investigative research\n",
    "This is the investigative research section 2019 yes.\n",
    "4 Results and analysis\n",
    "This is the results and analysis section.\n",
    "4.1 Magic act # 1: Hidden in plain sight: When partnerships are not partnerships\n",
    "This is subsection 4.1.\n",
    "4.2 Magic act #2: The disappearing act\n",
    "This is subsection 4.2.\n",
    "4.3 Magic act # 3: Spinning straw into gold\n",
    "This is subsection 4.3.\n",
    "4.4 Magic act # 4 voodoo economics: How costs are hidden or transformed into benefits\n",
    "This is subsection 4.4.\n",
    "4.5 Magic act # 5: Ghosting the numbers by falsifying and fudging\n",
    "This is subsection 4.5.\n",
    "5 Conclusion\n",
    "This is the conclusion section.\n",
    "References\n",
    "Some references.\n",
    "\"\"\"\n",
    "\n",
    "# List of section numbers and names\n",
    "sections = ['1 Introduction', '2 Background: Contextualising the Newcastle 500 from a political economy perspective', '3 Investigative research', '4 Conclusion']\n",
    "\n",
    "# Initialize a dictionary to store section contents\n",
    "section_contents = {}\n",
    "\n",
    "# Iterate over sections\n",
    "for section in sections:\n",
    "    # Escape special characters in section name\n",
    "    section_name = re.escape(section)\n",
    "    \n",
    "    # Create a regex pattern to find the section content\n",
    "    pattern = r\"{}([\\s\\S]*?)(?=\\d+\\s|$)\".format(section_name)\n",
    "    \n",
    "    # Find all matches for the section content using regex\n",
    "    matches = re.finditer(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # Extract the section content from each match\n",
    "    content = '\\n'.join(match.group().strip() for match in matches)\n",
    "    \n",
    "    # Store the section content in the dictionary\n",
    "    section_contents[section] = content\n",
    "\n",
    "# Print the section contents\n",
    "for section, content in section_contents.items():\n",
    "    print(section)\n",
    "    print(content)\n",
    "    print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
