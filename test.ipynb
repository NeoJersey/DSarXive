{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "openai.api_key = os.getenv(\"API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = {\n",
    "        \"data source\":[\n",
    "            {\n",
    "                \"name\": \"Twitter\"\n",
    "            }\n",
    "        ],\n",
    "        \"method\": \"Crawling\"\n",
    "}\n",
    "\n",
    "def extract(para):\n",
    "    prompt = f\"\"\"\n",
    "    based on the paragraph below, find the information and output into a JSON with keys \"data source\", \"method\". if you don't know the answer to a key, output \"unknown\". Please Make sure that all property names within your JSON are enclosed in double quotes.\n",
    "\n",
    "    The paragraph:\n",
    "    {para}\n",
    "\n",
    "    The JSON format:\n",
    "    {format}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",  # Use GPT-3.5 Turbo\n",
    "    prompt=prompt,\n",
    "    max_tokens=100,  # Adjust the max_tokens parameter as needed\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.7,  # Adjust the temperature parameter for desired response randomness\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].text.strip()\n",
    "    print(answer)\n",
    "    answer = json.loads(answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data source\": \"online questionnaire\",\n",
      "    \"method\": \"quantitative research\"\n",
      "    }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data source': 'online questionnaire', 'method': 'quantitative research'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = \"\"\"\n",
    "Methods Aiming to fulfill the objective to know how players perceive the chemistry in NMS, a quantitative research method, through an online questionnaire was implemented. Creswell (2003) states “quantitative research employ strategies of inquiry such as experimental surveys and collect data on predetermined instruments that yield statistical data” (p.18). Sample The questionnaire that will be better described below was implemented during the two last weeks of February 2021, when Hello Games launched a new update, which is an occasion when the number of active players increases. The questionnaire was published in three Facebook groups, in the NMS forum on Steam, and in the subreddit dedicated to the videogame. We’ve tried to reach as many players as possible, from different ages, and platforms. We concluded the data collection with 126 answers from 23 different countries. Questionnaire The questionnaire, done in Google forms, consists of 18 questions in total (ten of multiple choice, six of dichotomous answers, and two open answers that were prompted after certain choices) divided into four sections. The first one is with three questions to know more about the profile of the player (age, country, and the number of hours played of NMS). Then one section focused on the chemistry in the NMS and how players perceived it, with seven questions. And another five questions about science in videogames, with one open about what they learned – if they’ve learned something - while playing. And the final section with three questions, one open, about the chemistry in their day-to-day life. In questions 5, 6, 7, 8, 10, 11, and 12 a scale of values was presented (Likert, 1932) from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "answer = extract(para)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a = json.dumps(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m json\u001b[39m.\u001b[39;49mloads(\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata source\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m: [\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mProfessional Technology Temple (PTT)\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mFacebook\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mprofessional forum in Taiwan\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m], \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmethod\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m: \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mconvenience sampling\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m}\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "json.loads(\"{'data source': ['Professional Technology Temple (PTT)', 'Facebook', 'professional forum in Taiwan'], 'method': 'convenience sampling'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3cmd get --requester-pays s3://arxiv/src/arXiv_src_manifest.xml local-directory/arXiv_src_manifest.xml\n",
    "\n",
    "python download.py --manifest_file src/arXiv_src_2305_002.tar--mode src --output_dir local-directory/src/arXiv_src_2305_002.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadError",
     "evalue": "file could not be opened successfully:\n- method gz: ReadError('not a gzip file')\n- method bz2: ReadError('not a bzip2 file')\n- method xz: ReadError('not an lzma file')\n- method tar: ReadError('truncated header')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[39mfor\u001b[39;00m current_member \u001b[39min\u001b[39;00m current_members:\n\u001b[1;32m     26\u001b[0m                     chunk_tar\u001b[39m.\u001b[39maddfile(tarinfo\u001b[39m=\u001b[39mcurrent_member, fileobj\u001b[39m=\u001b[39mtar\u001b[39m.\u001b[39mextractfile(current_member))\n\u001b[0;32m---> 29\u001b[0m split_tar_file(\u001b[39m'\u001b[39;49m\u001b[39m/Users/jersey/Downloads/unarXive_230324_open_subset.tar.xz\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39moutput_file\u001b[39;49m\u001b[39m'\u001b[39;49m, chunk_size\u001b[39m=\u001b[39;49m\u001b[39m102400\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[67], line 4\u001b[0m, in \u001b[0;36msplit_tar_file\u001b[0;34m(input_file, output_prefix, chunk_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_tar_file\u001b[39m(input_file, output_prefix, chunk_size):\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mwith\u001b[39;00m tarfile\u001b[39m.\u001b[39;49mopen(input_file) \u001b[39mas\u001b[39;00m tar:\n\u001b[1;32m      5\u001b[0m         chunk_num \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      6\u001b[0m         current_chunk_size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/tarfile.py:1629\u001b[0m, in \u001b[0;36mTarFile.open\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m     error_msgs_summary \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m-> 1629\u001b[0m     \u001b[39mraise\u001b[39;00m ReadError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfile could not be opened successfully:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merror_msgs_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1631\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1632\u001b[0m     filemode, comptype \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mReadError\u001b[0m: file could not be opened successfully:\n- method gz: ReadError('not a gzip file')\n- method bz2: ReadError('not a bzip2 file')\n- method xz: ReadError('not an lzma file')\n- method tar: ReadError('truncated header')"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def split_tar_file(input_file, output_prefix, chunk_size):\n",
    "    with tarfile.open(input_file, 'r') as tar:\n",
    "        chunk_num = 1\n",
    "        current_chunk_size = 0\n",
    "        current_members = []\n",
    "\n",
    "        for member in tar.getmembers():\n",
    "            if current_chunk_size + member.size <= chunk_size:\n",
    "                current_members.append(member)\n",
    "                current_chunk_size += member.size\n",
    "            else:\n",
    "                output_file = f'{output_prefix}_{chunk_num}.tar'\n",
    "                with tarfile.open(output_file, 'w') as chunk_tar:\n",
    "                    for current_member in current_members:\n",
    "                        chunk_tar.addfile(tarinfo=current_member, fileobj=tar.extractfile(current_member))\n",
    "                current_members = [member]\n",
    "                current_chunk_size = member.size\n",
    "                chunk_num += 1\n",
    "\n",
    "        if current_members:\n",
    "            output_file = f'{output_prefix}_{chunk_num}.tar'\n",
    "            with tarfile.open(output_file, 'w') as chunk_tar:\n",
    "                for current_member in current_members:\n",
    "                    chunk_tar.addfile(tarinfo=current_member, fileobj=tar.extractfile(current_member))\n",
    "\n",
    "\n",
    "split_tar_file('/Users/jersey/Downloads/unarXive_230324_open_subset.tar.xz', 'output_file', chunk_size=102400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfile\n",
    "import tarfile\n",
    "\n",
    "with splitfile.open('data.bin', 'wb', volume_size=1000000) as f:\n",
    "    with tarfile.open(mode='w', fileobj=f) as t:\n",
    "        for file in files:\n",
    "            t.add(file)\n",
    "\n",
    "Result:\n",
    "data.bin\n",
    "data.bin.2\n",
    "data.bin.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
